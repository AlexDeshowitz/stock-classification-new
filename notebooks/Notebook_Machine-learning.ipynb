{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# visualization tools:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# models:\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# evaluation functions:\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook goal: Setup a basic machine learning framework that cleans data, standardizes features,\n",
    "#  evaluates feature impt, shap values, and a myriad of ML algorithms\n",
    "# TODO: add the day-of-week as a feature\n",
    "# TODO: Add in target date versus historic reference dates\n",
    "# TODO: Add in volume-based feature functionality\n",
    "# TODO: Evaluate standardizing features per stock or one model per stock - may not be enough data realistically\n",
    "# TODO: Check bol-range-pct calculation - only giving zero value\n",
    "# TODO: Add profit point forecast\n",
    "# TODO: Add trailing % change numbers to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions:\n",
    "\n",
    "def clean_stock_data(dataframe: pd.DataFrame) -> pd.DataFrame :\n",
    "\n",
    "    '''removes nulls and in the future will be built out to do any additonal cleaning on the dataframe that is necessary\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing all of the potential features\n",
    "        parameters: \n",
    "            calculation_field: field on which all of the features are built\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataset that is ready to load into a machine learning framework\n",
    "    '''\n",
    "\n",
    "    #TODO: In pipeline write this output to the catalogue\n",
    "    # remove records the preceed the target period to have complete information:\n",
    "    dataframe = dataframe.dropna() \n",
    "    #dataframe = dataframe.reset_index(drop = True) # we won't reset the index for now for traceability back to the date, ticker combination later after training\n",
    "\n",
    "    # set the date as an index to us post-forecasting: This is a bad idea, come back to the concept\n",
    "    #dataframe.set_index(keys = 'date', verify_integrity = False, inplace = True) # verify integrity Fale to allow duplicates**\n",
    "    \n",
    "    # remove fields that will not be used as predictive features (can be hardcoded since dataframe structure will be the same):\n",
    "    dataframe = dataframe.drop(columns = [ 'date', 'high', 'low', 'open', 'volume', 'adj_close'])\n",
    "    \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def identify_fields_to_standardize(dataframe: pd.DataFrame, parameters: Dict) -> np.array :\n",
    "\n",
    "    '''creates a list of the continuous fields to standardize by dimension within the predictive model; NOTE: this is used within the standardizer\n",
    "    \n",
    "    Args:\n",
    "        dataframe: dataframe that contains all of the fields of interest to be used in the calculations\n",
    "        parameters:\n",
    "            continuous_feature_cutoff: ratio of unique values to record count to be used to codify continuous features -> removes records from the standardization process which don't have enough data to standardize (e.g., boolean)\n",
    "\n",
    "    Returns: list of continuous fields to use in the standardization process based on user's specifications of \"uniqueness\" threshold    \n",
    "\n",
    "    '''\n",
    "\n",
    "    numeric_fields = dataframe.select_dtypes(include = 'number').columns\n",
    "    records = len(dataframe)\n",
    "\n",
    "    record_summary = pd.DataFrame(dataframe[numeric_fields].nunique(), columns = ['unique_values'])\n",
    "    record_summary['rows_in_df'] = records\n",
    "    record_summary['value_to_record_ratio'] = record_summary['unique_values']/ record_summary['rows_in_df']\n",
    "\n",
    "    # filter for a threshold specified by the user:\n",
    "    record_summary = record_summary[record_summary['value_to_record_ratio'] > parameters['continuous_feature_cutoff']]\n",
    "\n",
    "    # remove percentage features # TODO: later add in functionality to remove percentage based features\n",
    "\n",
    "    return record_summary.index\n",
    "\n",
    "\n",
    "# Justification for approach on scaling - the argument can be made that since our approach will generalize movemements across multiple securities that we need to standardize each security to its own price range.  Therefore, any features with price-relative values will be scaled per the security's price values to avoid odd splits in tree-based algos\n",
    "# the concern with standardization is generally focused on not letting any one feature have considerably more weight in a model than another; however in this case, \n",
    "\n",
    "\n",
    "def standardize_continuous_features(dataframe: pd.DataFrame, parameters: Dict) -> pd.DataFrame:\n",
    "\n",
    "    '''function that identifies the continuious features in the dataframe and standardizes each feature by equity to enable scaling relative to each equity\n",
    "    \n",
    "    Args:\n",
    "        Dataframe: Pandas dataframe to be used in machine learning\n",
    "        Parameters:\n",
    "            stock_field: field indicating the stock for the window function to scan\n",
    "            calculation_field: field for which the target is being calculated (used for drop in main row merge)\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe: containing the standardized data fields\n",
    "    \n",
    "    '''\n",
    "\n",
    "    continuous_fields = list(identify_fields_to_standardize(dataframe = dataframe, parameters = parameters))\n",
    "\n",
    "    # add in the ticker for grouping next:\n",
    "    continuous_fields.append(parameters['stock_field'])\n",
    "\n",
    "    # downselect to the fields that will be used to standardize:\n",
    "    continuous_dataframe = dataframe[continuous_fields]\n",
    "\n",
    "    # calculate z-scores: --> Standardizes within each feature to scale accordingly\n",
    "    z_scores = (continuous_dataframe - continuous_dataframe.groupby(by = parameters['stock_field']).transform('mean')) / continuous_dataframe.groupby(by = parameters['stock_field']).transform('std')\n",
    "\n",
    "    # drop the null ticker (not needed post groupby): \n",
    "    z_scores.drop(columns = [ parameters['stock_field'], parameters['calculation_field'] ], inplace = True)\n",
    "\n",
    "    # rename the fields to indicate standardization:\n",
    "    z_scores.columns = z_scores.columns + '_standardized'\n",
    "\n",
    "    # drop original continuous fields # TODO: coming back after calculation checks:\n",
    "    if parameters['drop_original_fields'] == True:\n",
    "        continuous_fields.remove(parameters['stock_field'])\n",
    "        dataframe.drop(columns = continuous_fields, inplace = True)\n",
    "\n",
    "    # append the fields back into the core dataframe:\n",
    "    z_scores = pd.concat([dataframe, z_scores], axis = 1)\n",
    "\n",
    "    # remove the standardized target field:\n",
    "    z_scores.drop(columns = z_scores.columns[z_scores.columns.str.contains('target')][1], inplace = True)\n",
    "\n",
    "    # remove unnecessary items:\n",
    "    del continuous_fields, continuous_dataframe\n",
    "\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode_tickers(dataframe: pd.DataFrame, parameters: Dict) -> pd.DataFrame:\n",
    "\n",
    "    '''Returns one-hot encoded features to the predictive dataset NOTE: May not work, but this retains some of the information in the original dataframe while also potentially giving the global model a nudge\n",
    "       Note: we choose not to drop first for now, even though it's a trap; Can be used post processing or as model features\n",
    "    Args:\n",
    "        dataframe: core dataset that has been augmented with additional features\n",
    "        parameters:\n",
    "            stock_field: text field containing the 3 letter ticker of the dataset\n",
    "    Returns:   \n",
    "        dataframe with augmented columns\n",
    "    \n",
    "    '''\n",
    "\n",
    "    dataframe = pd.get_dummies(data = dataframe, prefix = \"ind\", columns = [parameters['stock_field']], drop_first = False)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def profile_target_variable(dataframe: pd.DataFrame, parameters: dict):\n",
    "\n",
    "\n",
    "    '''Function that looks at the target variable and creates an output for the user to review and decide whether rebalancing will help classification task\n",
    "    Args:\n",
    "        dataframe: Main resulting dataframe from all data conversion steps\n",
    "        parameters:\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    # isolate the target variable:\n",
    "    target_field = list(dataframe.columns[dataframe.columns.str.contains('target')])\n",
    "\n",
    "    # create simple value count outputs:\n",
    "    target_summary_table = pd.DataFrame(dataframe[target_field].value_counts()).reset_index()\n",
    "    target_summary_table.rename(columns = {0 : 'counts'}, inplace = True)\n",
    "    target_summary_table['proportion'] = target_summary_table['counts'] / target_summary_table['counts'].sum()\n",
    "\n",
    "    # create bargraph and save it:\n",
    "    ''' TODO : resolve ability to output a matplotlib plot in kedro catalog\n",
    "    sns.countplot(x=target_field, data=dataframe)\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.show() '''\n",
    "    target_field =', '.join(dataframe.columns[dataframe.columns.str.contains('target')].str.replace(r'\\[|\\]', ''))\n",
    "    positive_proportion = target_summary_table[target_summary_table[target_field].astype(int) == 1]['proportion'].to_list()\n",
    "   \n",
    "\n",
    "    print('Classification target: ' + str(target_field) + \" contains a class balance of: \" + str(positive_proportion) + \" in the positive case\")\n",
    "           \n",
    "\n",
    "    return target_summary_table # TODO: Write this to the catalogue as a reporting output for the users\n",
    "\n",
    "\n",
    "\n",
    "def create_training_test_splits(dataframe: pd.DataFrame, parameters: Dict) :\n",
    "\n",
    "    '''Function that splits out training and test sets for machine learning; for the purposes of this model the way we piose the problem allows for random train test split\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing only the target field and the features to be used by the classifier\n",
    "        parameters:\n",
    "            test_ratio: proportion of samples in the dataframe to be used as a test set once the models are tuned and evaluated\n",
    "    \n",
    "    Returns:\n",
    "        X_train: training set for use in ML process\n",
    "        X_test: test set to be held out until all cross-validation is completed\n",
    "        y_train: training set for target variables\n",
    "        y_test: test target to be held out until all cross-validation is completed\n",
    "\n",
    "    '''\n",
    "\n",
    "    # define Y and x:\n",
    "    target_feature = list(dataframe.columns[dataframe.columns.str.contains('target')])\n",
    "\n",
    "    y = dataframe[target_feature]\n",
    "    X = dataframe.drop(columns = target_feature)\n",
    "\n",
    "    # create the training and test splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=parameters['test_size'], random_state=parameters['seed'], stratify = y)\n",
    "\n",
    "    #y_train = y_train.values.ravel()\n",
    "    #y_test = y_test.values.ravel()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def custom_recall_score(confusion_matrix: np.array) -> np.int64 :\n",
    "\n",
    "    recall_value = confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[1,0])\n",
    "\n",
    "    return recall_value\n",
    "\n",
    "\n",
    "def custom_precision_score(confusion_matrix: np.array) -> np.int64 : \n",
    "\n",
    "    precision_value = confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1])\n",
    "\n",
    "    return precision_value\n",
    "\n",
    "\n",
    "def extract_feature_importances(X: np.array, model: BaseEstimator) -> pd.DataFrame:\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract feature importances: \n",
    "    feature_importances = model.feature_importances_\n",
    "    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "                                          'Feature': feature_names,\n",
    "                                          'Importance': feature_importances\n",
    "                                        }\n",
    "                                        ).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "\n",
    "# dynamic import used in the model training pipeline:\n",
    "def dynamic_import(class_path: str):\n",
    "    \"\"\"Dynamically imports a class from its full path.\"\"\"\n",
    "    module_name, class_name = class_path.rsplit('.', 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)\n",
    "\n",
    "\n",
    "def get_roc_auc_score(y_pred: np.array, y_true: np.array, return_plot: bool) -> np.array:\n",
    "\n",
    "    '''Function that returns the auc of the model being run with the functionality to \n",
    "    return the graph output which will be stored in Kedro outputs\n",
    "    \n",
    "        Args:\n",
    "    - y_pred: predictions from the machine learning model\n",
    "    - y_actual: actual values being predicted by the machine learning model\n",
    "    \n",
    "    Returns:\n",
    "    - auc: area under the curve; A higher AUC indicates better classification performance\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # calculate the tpr and fpr for the different threshold values of the classifier outputs: \n",
    "    fpr, tpr, thresholds = roc_curve(y_true = y_true, y_score = y_pred)\n",
    "\n",
    "    # AUC calculation:\n",
    "    auc_output = auc( x = fpr, y = tpr )\n",
    "\n",
    "    if return_plot == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {auc_output:.2f})')\n",
    "        ax.plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--')  # Random guess line\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC Curve' + str(clf))\n",
    "        ax.legend(loc='lower right')\n",
    "\n",
    "        return fig, auc_output\n",
    "    \n",
    "    else:\n",
    "        return auc_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new model training function written (1/25):\n",
    "def train_models(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series, parameters: dict) -> pd.DataFrame:\n",
    "    \n",
    "    '''WRITE DOCUMENTATION'''\n",
    "    \n",
    "    # Convert y_train to a 1D array if it's a DataFrame\n",
    "    #y_train = y_train.iloc[:, 0].values if isinstance(y_train, pd.DataFrame) else y_train.values\n",
    "    y_train = y_train.values.ravel() if isinstance(y_train, pd.DataFrame) else y_train.ravel()\n",
    "    # Store feature names from the DataFrame\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    model, classifier_details, fold = [], [], []\n",
    "    train_aucs, test_aucs = [], []\n",
    "    train_precisions, test_precisions = [], []\n",
    "    train_recalls, test_recalls = [], []\n",
    "    train_f_scores, test_f_scores = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    train_true_positives, test_true_positives = [], []\n",
    "    train_true_negatives, test_true_negatives = [], []\n",
    "    train_false_positives, test_false_positives = [], []\n",
    "    train_false_negatives, test_false_negatives = [], []\n",
    "    \n",
    "    # Initialize feature importance storage\n",
    "    feature_importances_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through classifiers specified in the parameters\n",
    "    for clf_name, clf_info in parameters['classifiers'].items():\n",
    "        # Dynamically import and instantiate classifier\n",
    "        clf_class = dynamic_import(clf_info['class_path'])\n",
    "        clf_params = clf_info['params']\n",
    "        clf = clf_class(**clf_params)  # Initialize classifier with params\n",
    "\n",
    "        print(f'Training: {clf_name} classifier')\n",
    "\n",
    "        # Cross-validation loop\n",
    "        cv = StratifiedKFold(n_splits=parameters['cross_val_splits'], shuffle=True, random_state=parameters['seed']).split(X_train, y_train)\n",
    "\n",
    "        for k, (fold_train, fold_test) in enumerate(cv):\n",
    "            #clf.fit(X_train.iloc[fold_train], y_train[fold_train])\n",
    "            clf.fit(X_train.iloc[fold_train], y_train[fold_train])  # Keep DataFrame format\n",
    "            train_pred = clf.predict(X_train.iloc[fold_train])           \n",
    "            test_pred = clf.predict(X_train.iloc[fold_test])  \n",
    "            \n",
    "            # Predictions (at the 50% threshold)\n",
    "            train_pred = clf.predict(X_train.iloc[fold_train])\n",
    "            test_pred = clf.predict(X_train.iloc[fold_test])\n",
    "\n",
    "            \n",
    "            # AUC \n",
    "            if hasattr(clf, \"predict_proba\"): # Note: error, means predict proba exists, but must be set to true in params\n",
    "                train_pred_proba = clf.predict_proba(X_train.iloc[fold_train])[:, 1]\n",
    "                test_pred_proba = clf.predict_proba(X_train.iloc[fold_test])[:, 1] \n",
    "\n",
    "                train_auc = get_roc_auc_score(y_pred = train_pred_proba, y_true = y_train[fold_train], return_plot = False)\n",
    "                test_auc = get_roc_auc_score(y_pred = test_pred_proba, y_true = y_train[fold_test], return_plot = False)\n",
    "\n",
    "            else:\n",
    "                train_auc = np.nan\n",
    "                test_auc = np.nan\n",
    "            \n",
    "            \n",
    "            # Confusion matrices\n",
    "            train_confusion_matrix = confusion_matrix(y_train[fold_train], train_pred)\n",
    "            test_confusion_matrix = confusion_matrix(y_train[fold_test], test_pred)\n",
    "\n",
    "            # Accuracy\n",
    "            train_accuracy = clf.score(X_train.iloc[fold_train], y_train[fold_train])\n",
    "            test_accuracy = clf.score(X_train.iloc[fold_test], y_train[fold_test])\n",
    "\n",
    "            # Precision\n",
    "            train_precision = precision_score(y_train[fold_train], train_pred)\n",
    "            test_precision = precision_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # Recall\n",
    "            train_recall = recall_score(y_train[fold_train], train_pred)\n",
    "            test_recall = recall_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # F1 Score\n",
    "            train_f = f1_score(y_train[fold_train], train_pred)\n",
    "            test_f = f1_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # True/False Positives/Negatives\n",
    "            train_tp = train_confusion_matrix[1,1]\n",
    "            test_tp = test_confusion_matrix[1,1]\n",
    "            train_tn = train_confusion_matrix[0,0]\n",
    "            test_tn = test_confusion_matrix[0,0]\n",
    "            train_fp = train_confusion_matrix[0,1]\n",
    "            test_fp = test_confusion_matrix[0,1]\n",
    "            train_fn = train_confusion_matrix[1,0]\n",
    "            test_fn = test_confusion_matrix[1,0]\n",
    "\n",
    "            # Append metrics\n",
    "            model.append(clf_name)\n",
    "            classifier_details.append(clf)\n",
    "            fold.append(k)\n",
    "            train_aucs.append(train_auc)\n",
    "            test_aucs.append(test_auc)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            train_precisions.append(train_precision)\n",
    "            test_precisions.append(test_precision)\n",
    "            train_recalls.append(train_recall)\n",
    "            test_recalls.append(test_recall)\n",
    "            train_f_scores.append(train_f)\n",
    "            test_f_scores.append(test_f)\n",
    "            train_true_positives.append(train_tp)\n",
    "            test_true_positives.append(test_tp)\n",
    "            train_true_negatives.append(train_tn)\n",
    "            test_true_negatives.append(test_tn)\n",
    "            train_false_positives.append(train_fp)\n",
    "            test_false_positives.append(test_fp)\n",
    "            train_false_negatives.append(train_fn)\n",
    "            test_false_negatives.append(test_fn)\n",
    "\n",
    "            # Extract feature importances or coefficients\n",
    "            if hasattr(clf, \"coef_\"):  # For LogisticRegression and other linear models\n",
    "                feature_importances = clf.coef_.flatten()\n",
    "                temp_df = pd.DataFrame({\n",
    "                    \"model\": [clf_name]*len(feature_importances),\n",
    "                    \"fold\": [k]*len(feature_importances),\n",
    "                    \"feature\": feature_names,\n",
    "                    \"importance\": feature_importances\n",
    "                })\n",
    "                feature_importances_df = pd.concat([feature_importances_df, temp_df], ignore_index=True)\n",
    "            \n",
    "            elif hasattr(clf, \"feature_importances_\"):  # For RandomForest, XGBoost, and other ensemble/non-linear models\n",
    "                feature_importances = clf.feature_importances_\n",
    "                temp_df = pd.DataFrame({\n",
    "                    \"model\": [clf_name]*len(feature_importances),\n",
    "                    \"fold\": [k]*len(feature_importances),\n",
    "                    \"feature\": feature_names,\n",
    "                    \"importance\": feature_importances\n",
    "                })\n",
    "                feature_importances_df = pd.concat([feature_importances_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Summary of feature importances\n",
    "    feature_importances_summary_df = feature_importances_df.groupby(['model', 'feature']).agg(\n",
    "        importance_value=('importance', 'mean')).reset_index()\n",
    "\n",
    "    # Detailed results DataFrame\n",
    "    detailed_results_df = pd.DataFrame({\n",
    "        \"model\": model,\n",
    "        \"classifier_details\": classifier_details,\n",
    "        \"fold\": fold,\n",
    "        \"train_auc\" : train_aucs,\n",
    "        \"test_auc\" : test_aucs,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"test_accuracy\": test_accuracies,\n",
    "        \"train_precision\": train_precisions,\n",
    "        \"test_precision\": test_precisions,\n",
    "        \"train_recall\": train_recalls,\n",
    "        \"test_recall\": test_recalls,\n",
    "        \"train_true_positives\": train_true_positives,\n",
    "        \"test_true_positives\": test_true_positives,\n",
    "        \"train_true_negatives\": train_true_negatives,\n",
    "        \"test_true_negatives\": test_true_negatives,\n",
    "        \"train_false_positives\": train_false_positives,\n",
    "        \"test_false_positives\": test_false_positives,\n",
    "        \"train_false_negatives\": train_false_negatives,\n",
    "        \"test_false_negatives\": test_false_negatives\n",
    "    })\n",
    "\n",
    "    # Aggregated results\n",
    "    results_df = detailed_results_df.groupby(by = ['model']).agg({\n",
    "        'train_auc' : 'mean',\n",
    "        'test_auc' : 'mean',\n",
    "        'train_accuracy': 'mean',\n",
    "        'test_accuracy': 'mean',\n",
    "        'train_precision': 'mean',\n",
    "        'test_precision': 'mean',\n",
    "        'train_recall': 'mean',\n",
    "        'test_recall': 'mean',\n",
    "        'train_true_positives': 'sum',\n",
    "        'test_true_positives': 'sum',\n",
    "        'train_true_negatives': 'sum',\n",
    "        'test_true_negatives': 'sum',\n",
    "        'train_false_positives': 'sum',\n",
    "        'test_false_positives': 'sum',\n",
    "        'train_false_negatives': 'sum',\n",
    "        'test_false_negatives': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Add in overall positve-class rates to the datasets:\n",
    "    results_df['train_positive_rate'] = (results_df['train_true_positives'] + results_df['train_false_negatives']) / (results_df['train_true_positives'] + results_df['train_false_negatives'] + results_df['train_false_positives'] + results_df['train_true_negatives'])\n",
    "    results_df['test_positive_rate'] = (results_df['test_true_positives'] + results_df['test_false_negatives']) / (results_df['test_true_positives'] + results_df['test_false_negatives'] + results_df['test_false_positives'] + results_df['test_true_negatives'])\n",
    "\n",
    "    # share message with complete:\n",
    "    print('------------------')\n",
    "    print('Training evaluation completed')\n",
    "    print('------------------')\n",
    "\n",
    "    return detailed_results_df, results_df, feature_importances_df, feature_importances_summary_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_champion_model(models: pd.DataFrame, parameters: str):\n",
    "    \"\"\"\n",
    "    Selects the champion model based on the highest or lowest value of a given optimization target.\n",
    "    \n",
    "    Args:\n",
    "    - models: A DataFrame containing model performance metrics.\n",
    "    - optimization_target: The metric by which to select the champion model (e.g., 'test_precision', 'test_recall', 'test_false_positives').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_model: The row corresponding to the selected champion model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if parameters['optimization_target'] not in models.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target'] }' not found in model metrics. Choose from: {list(models.columns)}\")\n",
    "    \n",
    "    # Determine whether we want the highest or lowest value\n",
    "    # Assuming metrics like 'precision', 'recall', 'accuracy' need the highest, and 'false positives', 'false negatives', etc. need the lowest\n",
    "    if 'precision' in parameters['optimization_target']  or 'recall' in parameters['optimization_target']  or 'accuracy' in parameters['optimization_target'] :\n",
    "        ascending = False  # We want to maximize these metrics\n",
    "   \n",
    "    else:\n",
    "        ascending = True  # For counts (e.g., false positives), we want the minimum value\n",
    "    \n",
    "    # Sort the models based on the optimization target\n",
    "    sorted_models = models.sort_values(by=parameters['optimization_target'] , ascending=ascending)\n",
    "    \n",
    "    # Return the top model (first row after sorting)\n",
    "    champion_model = pd.DataFrame(sorted_models.iloc[0]).reset_index()\n",
    "\n",
    "    champion_model.columns = ['element', 'value']\n",
    "    \n",
    "\n",
    "    return champion_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = catalog.load('combined_modeling_input')\n",
    "df = pd.read_csv('../data/03_primary/combined_modeling_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the parameters for the model: \n",
    "\n",
    "parameters = {'continuous_feature_cutoff' : 0.6,\n",
    "              'stock_field' : 'ticker',\n",
    "              'calculation_field' : 'close',\n",
    "              'drop_original_fields' : True,\n",
    "              'drop_stock_field': True, # keep this fixed \n",
    "              'test_size' : 0.20, # proportion of the dataset held out as the test set\n",
    "              'seed' : 1187,\n",
    "              'cross_val_splits' : 5,\n",
    "              'c' : 1.0,\n",
    "              'kernel' : 'rbf',\n",
    "              'gamma' : 'scale',\n",
    "              'optimization_target' : 'test_recall', # other_useful_inputs: train_accuracy, test_accuracy, train_precision, train_recall, test_recall, true_positives, false_positives\n",
    "              'optimization_target_threshold' : 0.8, # minimum value of the optimization target needed to be selected as a final model\n",
    "              'optimization_target_training_threshold' : 0.1, # the maximum allowoable difference (absolute) between training and test optimization targets\n",
    "              'number_of_models' : 3, # the number of models to be \n",
    "              # specify the algorithms to be used:\n",
    "              'classifiers': {\n",
    "                'Logistic_regression': {\n",
    "                    'class_path': 'sklearn.linear_model.LogisticRegression',\n",
    "                    'params': {'penalty': 'l2', 'C': 1.0, 'max_iter': 100000}\n",
    "                },\n",
    "                'Random_forest': {\n",
    "                    'class_path': 'sklearn.ensemble.RandomForestClassifier',\n",
    "                    'params': {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
    "                },\n",
    "                'Support_vector_classifier': {\n",
    "                    'class_path': 'sklearn.svm.SVC',\n",
    "                    'params': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale', 'probability': True}\n",
    "                },\n",
    "                'XGBoost': {\n",
    "                    'class_path': 'xgboost.XGBClassifier',\n",
    "                    'params': {'use_label_encoder': False, 'eval_metric': 'logloss'}\n",
    "                },\n",
    "                'K_nearest_neighbors': {\n",
    "                    'class_path': 'sklearn.neighbors.KNeighborsClassifier',\n",
    "                    'params': {'n_neighbors': 5, 'weights': 'uniform'}\n",
    "                },\n",
    "                'Gradient_boosting': {\n",
    "                    'class_path': 'sklearn.ensemble.GradientBoostingClassifier',\n",
    "                    'params': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3}\n",
    "                },\n",
    "                'Naive_bayes': {\n",
    "                    'class_path': 'sklearn.naive_bayes.GaussianNB',\n",
    "                    'params': {}\n",
    "                },\n",
    "                'Balanced_random_forest': {\n",
    "                    'class_path': 'imblearn.ensemble.BalancedRandomForestClassifier',\n",
    "                    'params': {'n_estimators': 200, 'max_features': 'sqrt', 'sampling_strategy' : 'all', 'bootstrap' : False, 'replacement' : True}\n",
    "                },\n",
    "                'AdaBoost': {\n",
    "                    'class_path': 'sklearn.ensemble.AdaBoostClassifier',\n",
    "                    'params': {'n_estimators': 50, 'learning_rate': 1.0, 'algorithm' : 'SAMME'}\n",
    "                },\n",
    "                'CatBoost': {\n",
    "                    'class_path': 'catboost.CatBoostClassifier',\n",
    "                    'params': {'iterations': 100, 'learning_rate': 0.1, 'depth': 6, 'silent': True}\n",
    "                }\n",
    "            }\n",
    "\n",
    "}\n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Logistic_regression classifier\n",
      "Training: Random_forest classifier\n",
      "Training: Support_vector_classifier classifier\n",
      "Training: XGBoost classifier\n",
      "Training: K_nearest_neighbors classifier\n",
      "Training: Gradient_boosting classifier\n",
      "Training: Naive_bayes classifier\n",
      "Training: Balanced_random_forest classifier\n",
      "Training: AdaBoost classifier\n",
      "Training: CatBoost classifier\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'CatBoostClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m create_training_test_splits(dataframe\u001b[38;5;241m=\u001b[39mtest, parameters\u001b[38;5;241m=\u001b[39m parameters)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# train the models:\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m detailed_output, summary_output, feat_impt, feat_impt_summary \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 437\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(X_train, X_test, y_train, y_test, parameters)\u001b[0m\n\u001b[1;32m    414\u001b[0m detailed_results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier_details\u001b[39m\u001b[38;5;124m\"\u001b[39m: classifier_details,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_false_negatives\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_false_negatives\n\u001b[1;32m    434\u001b[0m })\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Aggregated results\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mdetailed_results_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_details\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_auc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_auc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_precision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_precision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_recall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_recall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_true_positives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_true_positives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_true_negatives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_true_negatives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_false_positives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_false_positives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_false_negatives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_false_negatives\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# Add in overall positve-class rates to the datasets:\u001b[39;00m\n\u001b[1;32m    457\u001b[0m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_positive_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_true_positives\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_false_negatives\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m (results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_true_positives\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_false_negatives\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_false_positives\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_true_negatives\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m ):\n\u001b[0;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/apply.py:496\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/apply.py:497\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    493\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 497\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gotitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/generic.py:249\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, abc\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# Catch instances of lists / tuples\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:2452\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   2446\u001b[0m         grouped_mean,\n\u001b[1;32m   2447\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[1;32m   2448\u001b[0m         engine_kwargs,\n\u001b[1;32m   2449\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2450\u001b[0m     )\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/internals/base.py:367\u001b[0m, in \u001b[0;36mSingleDataManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrouped_reduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func):\n\u001b[1;32m    366\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\n\u001b[0;32m--> 367\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(res))\n\u001b[1;32m    370\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_array(res, index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1973\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_func\u001b[39m(values: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1973\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maggregate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1977\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1978\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1979\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1980\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1982\u001b[0m         \u001b[38;5;66;03m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[1;32m   1983\u001b[0m         \u001b[38;5;66;03m# and non-applicable functions\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m         \u001b[38;5;66;03m# try to python agg\u001b[39;00m\n\u001b[1;32m   1985\u001b[0m         \u001b[38;5;66;03m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m         \u001b[38;5;66;03m# TODO: avoid special casing SparseArray here\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, SparseArray):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/ops.py:827\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[0;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03mReturns the values of a cython operation.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m kind \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 827\u001b[0m cy_op \u001b[38;5;241m=\u001b[39m WrappedCythonOp(kind\u001b[38;5;241m=\u001b[39mkind, how\u001b[38;5;241m=\u001b[39mhow, has_dropped_na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_dropped_na\u001b[49m)\n\u001b[1;32m    829\u001b[0m ids, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_info\n\u001b[1;32m    830\u001b[0m ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups\n",
      "File \u001b[0;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/ops.py:741\u001b[0m, in \u001b[0;36mBaseGrouper.has_dropped_na\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_dropped_na\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    738\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m    Whether grouper has null value(s) that are dropped.\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_info\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many())\n",
      "File \u001b[0;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/ops.py:745\u001b[0m, in \u001b[0;36mBaseGrouper.group_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgroup_info\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 745\u001b[0m     comp_ids, obs_group_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_compressed_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m     ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(obs_group_ids)\n\u001b[1;32m    748\u001b[0m     comp_ids \u001b[38;5;241m=\u001b[39m ensure_platform_int(comp_ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/ops.py:764\u001b[0m, in \u001b[0;36mBaseGrouper._get_compressed_codes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_compressed_codes\u001b[39m(\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    761\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39msignedinteger], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]]:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;66;03m# The first returned ndarray may have any signed integer dtype\u001b[39;00m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupings) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 764\u001b[0m         group_index \u001b[38;5;241m=\u001b[39m get_group_index(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodes\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xnull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    765\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m compress_group_index(group_index, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort)\n\u001b[1;32m    766\u001b[0m         \u001b[38;5;66;03m# FIXME: compress_group_index's second return value is int64, not intp\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/ops.py:690\u001b[0m, in \u001b[0;36mBaseGrouper.codes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39msignedinteger]]:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [ping\u001b[38;5;241m.\u001b[39mcodes \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupings]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/ops.py:690\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39msignedinteger]]:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodes\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupings]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:691\u001b[0m, in \u001b[0;36mGrouping.codes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39msignedinteger]:\n\u001b[0;32m--> 691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_codes_and_uniques\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:835\u001b[0m, in \u001b[0;36mGrouping._codes_and_uniques\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    830\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uniques\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# GH35667, replace dropna=False with use_na_sentinel=False\u001b[39;00m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type \"Union[\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;66;03m# ndarray[Any, Any], Index]\", variable has type \"Categorical\")\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m     codes, uniques \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouping_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dropna\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m codes, uniques\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/algorithms.py:795\u001b[0m, in \u001b[0;36mfactorize\u001b[0;34m(values, sort, use_na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[1;32m    793\u001b[0m             values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[0;32m--> 795\u001b[0m     codes, uniques \u001b[38;5;241m=\u001b[39m \u001b[43mfactorize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    802\u001b[0m     uniques, codes \u001b[38;5;241m=\u001b[39m safe_sort(\n\u001b[1;32m    803\u001b[0m         uniques,\n\u001b[1;32m    804\u001b[0m         codes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m         verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    808\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/pandas/core/algorithms.py:595\u001b[0m, in \u001b[0;36mfactorize_array\u001b[0;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    592\u001b[0m hash_klass, values \u001b[38;5;241m=\u001b[39m _get_hashtable_algo(values)\n\u001b[1;32m    594\u001b[0m table \u001b[38;5;241m=\u001b[39m hash_klass(size_hint \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values))\n\u001b[0;32m--> 595\u001b[0m uniques, codes \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[1;32m    604\u001b[0m uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7281\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7195\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'CatBoostClassifier'"
     ]
    }
   ],
   "source": [
    "# remove the null values from the dataset and drop un-needed columns for the classifier:\n",
    "test = clean_stock_data(dataframe= df)\n",
    "# test: standardize features:\n",
    "test = standardize_continuous_features(dataframe = test, parameters = parameters)\n",
    "# one-hot encode: \n",
    "test = one_hot_encode_tickers(dataframe = test, parameters= parameters)\n",
    "# create training and test sets\n",
    "X_train, X_test, y_train, y_test = create_training_test_splits(dataframe=test, parameters= parameters)\n",
    "# train the models:\n",
    "detailed_output, summary_output, feat_impt, feat_impt_summary = train_models(X_train = X_train, X_test = X_test, y_train = y_train, y_test= y_test, parameters = parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### - Function development HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new champion model selection function:\n",
    "\n",
    "def dynamic_champion_selection(modeling_results_summary: pd.DataFrame, parameters: dict) -> pd.DataFrame:\n",
    "\n",
    "    '''\n",
    "    Selects a champion model by optimizing for the user's specified inputs to include train/test minimization\n",
    "\n",
    "    Inputs:\n",
    "        modeling_results_summary: output of the initial model selection pipeline\n",
    "        parameters:\n",
    "            optimization_target: the KPI that is most important to the task at hand (e.g., test recall)\n",
    "            minimize_test/train: True or False indicating whether we want to instantiate a parameter for train/test accuracy\n",
    "            minimum_accuracy_threshold: Minimum value of the optimization target for it to be considered in top models\n",
    "            maximum_train_test_diff: Maximum amount differential between train and test value of the optimization target to be considered\n",
    "            maximum_number_of_models: Number of models to be selected for downstream champion\n",
    "\n",
    "    '''\n",
    "\n",
    "    if parameters['optimization_target'] not in modeling_results_summary.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target']} not found in model outputs. Choose from: {list(modeling_results_summary.columns)} \")\n",
    "    \n",
    "\n",
    "    # 1. assess the sort value needed to do the selection appropriately:\n",
    "    if 'precision' in parameters['optimization_target'] or 'recall' in parameters['optimization_target'] or 'accuracy'in parameters['optimization_target'] or 'auc' in parameters['optimization_target']:\n",
    "        ascending = False # Get the maximum for these types of measurements\n",
    "    else: # for counts\n",
    "        ascending = True # Get the minimum for these types of measurements\n",
    "\n",
    "    # 2. determine the compliment of the optimization parameter provided:\n",
    "    if 'train' in parameters['optimization_target']:\n",
    "        compliment = parameters['optimization_target'].replace('train', 'test')\n",
    "        print(parameters['optimization_target'])\n",
    "        print(compliment)\n",
    "    else: # for test case\n",
    "        compliment = parameters['optimization_target'].replace('test', 'train')\n",
    "        print(parameters['optimization_target'])\n",
    "        print(compliment)\n",
    "\n",
    "    # 3. Add the difference between the training and test columns:\n",
    "    modeling_results_summary['opt_train_test_difference'] = abs(modeling_results_summary[parameters['optimization_target']] - modeling_results_summary[compliment])\n",
    "\n",
    "    # 4. Sort the dataframe and instantiate one to return:\n",
    "    sorted_models = modeling_results_summary.sort_values(by = parameters['optimization_target'], ascending = ascending)\n",
    "\n",
    "    # 5. remove models not meeting the minimum threshold: \n",
    "    sorted_models = sorted_models[sorted_models[parameters['optimization_target']] >= parameters['optimization_target_threshold']].reset_index(drop = True)\n",
    "\n",
    "    # 6. optimize for the difference between train and test sets:\n",
    "    sorted_models = sorted_models[sorted_models['opt_train_test_difference'] <=  parameters['optimization_target_training_threshold']]\n",
    "\n",
    "    # 7. Select the top \"N\" models from the parameters specified:\n",
    "    sorted_models = sorted_models[0:parameters['number_of_models']].reset_index(drop = True)\n",
    "\n",
    "\n",
    "    #TODO: add step to pick the top N models and then a new function to call and retrieve the function specifications from the paramaters to run the next pipeline\n",
    "\n",
    "    return sorted_models\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_recall\n",
      "train_recall\n"
     ]
    }
   ],
   "source": [
    "sorted_models = dynamic_champion_selection(modeling_results_summary = summary_output, parameters = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_true_positives</th>\n",
       "      <th>test_true_positives</th>\n",
       "      <th>train_true_negatives</th>\n",
       "      <th>test_true_negatives</th>\n",
       "      <th>train_false_positives</th>\n",
       "      <th>test_false_positives</th>\n",
       "      <th>train_false_negatives</th>\n",
       "      <th>test_false_negatives</th>\n",
       "      <th>train_positive_rate</th>\n",
       "      <th>test_positive_rate</th>\n",
       "      <th>opt_train_test_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>0.700547</td>\n",
       "      <td>0.663099</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.630477</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.631684</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.972661</td>\n",
       "      <td>5156</td>\n",
       "      <td>1281</td>\n",
       "      <td>274</td>\n",
       "      <td>55</td>\n",
       "      <td>2934</td>\n",
       "      <td>747</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.006079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>0.977579</td>\n",
       "      <td>0.875844</td>\n",
       "      <td>0.911633</td>\n",
       "      <td>0.780553</td>\n",
       "      <td>0.896305</td>\n",
       "      <td>0.775655</td>\n",
       "      <td>0.970199</td>\n",
       "      <td>0.911148</td>\n",
       "      <td>5111</td>\n",
       "      <td>1200</td>\n",
       "      <td>2616</td>\n",
       "      <td>454</td>\n",
       "      <td>592</td>\n",
       "      <td>348</td>\n",
       "      <td>157</td>\n",
       "      <td>117</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.059051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.984055</td>\n",
       "      <td>0.887409</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.806981</td>\n",
       "      <td>0.914695</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.908866</td>\n",
       "      <td>5156</td>\n",
       "      <td>1197</td>\n",
       "      <td>2727</td>\n",
       "      <td>513</td>\n",
       "      <td>481</td>\n",
       "      <td>289</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.069874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  train_auc  test_auc  train_accuracy  \\\n",
       "0  Support_vector_classifier   0.700547  0.663099        0.640632   \n",
       "1              Random_forest   0.977579  0.875844        0.911633   \n",
       "2                   CatBoost   0.984055  0.887409        0.930038   \n",
       "\n",
       "   test_accuracy  train_precision  test_precision  train_recall  test_recall  \\\n",
       "0       0.630477         0.637371        0.631684      0.978740     0.972661   \n",
       "1       0.780553         0.896305        0.775655      0.970199     0.911148   \n",
       "2       0.806981         0.914695        0.805601      0.978740     0.908866   \n",
       "\n",
       "   train_true_positives  test_true_positives  train_true_negatives  \\\n",
       "0                  5156                 1281                   274   \n",
       "1                  5111                 1200                  2616   \n",
       "2                  5156                 1197                  2727   \n",
       "\n",
       "   test_true_negatives  train_false_positives  test_false_positives  \\\n",
       "0                   55                   2934                   747   \n",
       "1                  454                    592                   348   \n",
       "2                  513                    481                   289   \n",
       "\n",
       "   train_false_negatives  test_false_negatives  train_positive_rate  \\\n",
       "0                    112                    36              0.62152   \n",
       "1                    157                   117              0.62152   \n",
       "2                    112                   120              0.62152   \n",
       "\n",
       "   test_positive_rate  opt_train_test_difference  \n",
       "0             0.62152                   0.006079  \n",
       "1             0.62152                   0.059051  \n",
       "2             0.62152                   0.069874  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_models[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>classifier_details</th>\n",
       "      <th>fold</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_true_positives</th>\n",
       "      <th>test_true_positives</th>\n",
       "      <th>train_true_negatives</th>\n",
       "      <th>test_true_negatives</th>\n",
       "      <th>train_false_positives</th>\n",
       "      <th>test_false_positives</th>\n",
       "      <th>train_false_negatives</th>\n",
       "      <th>test_false_negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>LogisticRegression(max_iter=100000)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.674817</td>\n",
       "      <td>0.609328</td>\n",
       "      <td>0.645428</td>\n",
       "      <td>0.636792</td>\n",
       "      <td>0.660511</td>\n",
       "      <td>0.654494</td>\n",
       "      <td>0.883191</td>\n",
       "      <td>0.882576</td>\n",
       "      <td>930</td>\n",
       "      <td>233</td>\n",
       "      <td>164</td>\n",
       "      <td>37</td>\n",
       "      <td>478</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>LogisticRegression(max_iter=100000)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.657506</td>\n",
       "      <td>0.676420</td>\n",
       "      <td>0.634218</td>\n",
       "      <td>0.658019</td>\n",
       "      <td>0.651293</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.885090</td>\n",
       "      <td>0.901515</td>\n",
       "      <td>932</td>\n",
       "      <td>238</td>\n",
       "      <td>143</td>\n",
       "      <td>41</td>\n",
       "      <td>499</td>\n",
       "      <td>119</td>\n",
       "      <td>121</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>LogisticRegression(max_iter=100000)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.665182</td>\n",
       "      <td>0.651985</td>\n",
       "      <td>0.638938</td>\n",
       "      <td>0.636792</td>\n",
       "      <td>0.655197</td>\n",
       "      <td>0.654391</td>\n",
       "      <td>0.885199</td>\n",
       "      <td>0.878327</td>\n",
       "      <td>933</td>\n",
       "      <td>231</td>\n",
       "      <td>150</td>\n",
       "      <td>39</td>\n",
       "      <td>491</td>\n",
       "      <td>122</td>\n",
       "      <td>121</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>LogisticRegression(max_iter=100000)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.664156</td>\n",
       "      <td>0.650261</td>\n",
       "      <td>0.632448</td>\n",
       "      <td>0.632075</td>\n",
       "      <td>0.649965</td>\n",
       "      <td>0.652422</td>\n",
       "      <td>0.886148</td>\n",
       "      <td>0.870722</td>\n",
       "      <td>934</td>\n",
       "      <td>229</td>\n",
       "      <td>138</td>\n",
       "      <td>39</td>\n",
       "      <td>503</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>LogisticRegression(max_iter=100000)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.671178</td>\n",
       "      <td>0.641778</td>\n",
       "      <td>0.640920</td>\n",
       "      <td>0.612293</td>\n",
       "      <td>0.657913</td>\n",
       "      <td>0.632708</td>\n",
       "      <td>0.879507</td>\n",
       "      <td>0.897338</td>\n",
       "      <td>927</td>\n",
       "      <td>236</td>\n",
       "      <td>160</td>\n",
       "      <td>23</td>\n",
       "      <td>482</td>\n",
       "      <td>137</td>\n",
       "      <td>127</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.975755</td>\n",
       "      <td>0.868703</td>\n",
       "      <td>0.919764</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.904678</td>\n",
       "      <td>0.770440</td>\n",
       "      <td>0.973409</td>\n",
       "      <td>0.928030</td>\n",
       "      <td>1025</td>\n",
       "      <td>245</td>\n",
       "      <td>534</td>\n",
       "      <td>87</td>\n",
       "      <td>108</td>\n",
       "      <td>73</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980471</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>0.912684</td>\n",
       "      <td>0.780660</td>\n",
       "      <td>0.893821</td>\n",
       "      <td>0.774920</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.912879</td>\n",
       "      <td>1027</td>\n",
       "      <td>241</td>\n",
       "      <td>520</td>\n",
       "      <td>90</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', m...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.975421</td>\n",
       "      <td>0.873297</td>\n",
       "      <td>0.899115</td>\n",
       "      <td>0.761792</td>\n",
       "      <td>0.882251</td>\n",
       "      <td>0.761290</td>\n",
       "      <td>0.966793</td>\n",
       "      <td>0.897338</td>\n",
       "      <td>1019</td>\n",
       "      <td>236</td>\n",
       "      <td>505</td>\n",
       "      <td>87</td>\n",
       "      <td>136</td>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', m...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.976812</td>\n",
       "      <td>0.870297</td>\n",
       "      <td>0.918584</td>\n",
       "      <td>0.804245</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.964896</td>\n",
       "      <td>0.897338</td>\n",
       "      <td>1017</td>\n",
       "      <td>236</td>\n",
       "      <td>540</td>\n",
       "      <td>105</td>\n",
       "      <td>101</td>\n",
       "      <td>56</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', m...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.979435</td>\n",
       "      <td>0.886217</td>\n",
       "      <td>0.908019</td>\n",
       "      <td>0.773050</td>\n",
       "      <td>0.891115</td>\n",
       "      <td>0.763407</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.920152</td>\n",
       "      <td>1023</td>\n",
       "      <td>242</td>\n",
       "      <td>517</td>\n",
       "      <td>85</td>\n",
       "      <td>125</td>\n",
       "      <td>75</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>SVC(probability=True)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.703428</td>\n",
       "      <td>0.634197</td>\n",
       "      <td>0.638938</td>\n",
       "      <td>0.643868</td>\n",
       "      <td>0.636533</td>\n",
       "      <td>0.639506</td>\n",
       "      <td>0.976258</td>\n",
       "      <td>0.981061</td>\n",
       "      <td>1028</td>\n",
       "      <td>259</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>587</td>\n",
       "      <td>146</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>SVC(probability=True)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.695433</td>\n",
       "      <td>0.697455</td>\n",
       "      <td>0.644248</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.638718</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.983856</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>1036</td>\n",
       "      <td>256</td>\n",
       "      <td>56</td>\n",
       "      <td>8</td>\n",
       "      <td>586</td>\n",
       "      <td>152</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>SVC(probability=True)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.700718</td>\n",
       "      <td>0.676263</td>\n",
       "      <td>0.643068</td>\n",
       "      <td>0.636792</td>\n",
       "      <td>0.640752</td>\n",
       "      <td>0.636591</td>\n",
       "      <td>0.969639</td>\n",
       "      <td>0.965779</td>\n",
       "      <td>1022</td>\n",
       "      <td>254</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>573</td>\n",
       "      <td>145</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>SVC(probability=True)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.697995</td>\n",
       "      <td>0.675318</td>\n",
       "      <td>0.633038</td>\n",
       "      <td>0.636792</td>\n",
       "      <td>0.630751</td>\n",
       "      <td>0.633252</td>\n",
       "      <td>0.988615</td>\n",
       "      <td>0.984791</td>\n",
       "      <td>1042</td>\n",
       "      <td>259</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>610</td>\n",
       "      <td>150</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>SVC(probability=True)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.705160</td>\n",
       "      <td>0.632260</td>\n",
       "      <td>0.643868</td>\n",
       "      <td>0.612293</td>\n",
       "      <td>0.640100</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.975332</td>\n",
       "      <td>0.961977</td>\n",
       "      <td>1028</td>\n",
       "      <td>253</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>578</td>\n",
       "      <td>154</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1053</td>\n",
       "      <td>242</td>\n",
       "      <td>642</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917377</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.837264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>1053</td>\n",
       "      <td>236</td>\n",
       "      <td>642</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927757</td>\n",
       "      <td>1054</td>\n",
       "      <td>244</td>\n",
       "      <td>641</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.865566</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882129</td>\n",
       "      <td>1054</td>\n",
       "      <td>232</td>\n",
       "      <td>641</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.878229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904943</td>\n",
       "      <td>1054</td>\n",
       "      <td>238</td>\n",
       "      <td>642</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907910</td>\n",
       "      <td>0.777012</td>\n",
       "      <td>0.836578</td>\n",
       "      <td>0.752358</td>\n",
       "      <td>0.842756</td>\n",
       "      <td>0.780919</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.837121</td>\n",
       "      <td>954</td>\n",
       "      <td>221</td>\n",
       "      <td>464</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>62</td>\n",
       "      <td>99</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911921</td>\n",
       "      <td>0.741004</td>\n",
       "      <td>0.850147</td>\n",
       "      <td>0.693396</td>\n",
       "      <td>0.853853</td>\n",
       "      <td>0.726351</td>\n",
       "      <td>0.915480</td>\n",
       "      <td>0.814394</td>\n",
       "      <td>964</td>\n",
       "      <td>215</td>\n",
       "      <td>477</td>\n",
       "      <td>79</td>\n",
       "      <td>165</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>2</td>\n",
       "      <td>0.910367</td>\n",
       "      <td>0.747077</td>\n",
       "      <td>0.840708</td>\n",
       "      <td>0.702830</td>\n",
       "      <td>0.846903</td>\n",
       "      <td>0.747292</td>\n",
       "      <td>0.907970</td>\n",
       "      <td>0.787072</td>\n",
       "      <td>957</td>\n",
       "      <td>207</td>\n",
       "      <td>468</td>\n",
       "      <td>91</td>\n",
       "      <td>173</td>\n",
       "      <td>70</td>\n",
       "      <td>97</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>3</td>\n",
       "      <td>0.901063</td>\n",
       "      <td>0.774437</td>\n",
       "      <td>0.830678</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.837885</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.902277</td>\n",
       "      <td>0.851711</td>\n",
       "      <td>951</td>\n",
       "      <td>224</td>\n",
       "      <td>457</td>\n",
       "      <td>88</td>\n",
       "      <td>184</td>\n",
       "      <td>73</td>\n",
       "      <td>103</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>4</td>\n",
       "      <td>0.904422</td>\n",
       "      <td>0.763569</td>\n",
       "      <td>0.839623</td>\n",
       "      <td>0.716312</td>\n",
       "      <td>0.840592</td>\n",
       "      <td>0.744027</td>\n",
       "      <td>0.915560</td>\n",
       "      <td>0.828897</td>\n",
       "      <td>965</td>\n",
       "      <td>218</td>\n",
       "      <td>459</td>\n",
       "      <td>85</td>\n",
       "      <td>183</td>\n",
       "      <td>75</td>\n",
       "      <td>89</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955040</td>\n",
       "      <td>0.825272</td>\n",
       "      <td>0.864307</td>\n",
       "      <td>0.745283</td>\n",
       "      <td>0.838127</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>0.968661</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>1020</td>\n",
       "      <td>236</td>\n",
       "      <td>445</td>\n",
       "      <td>80</td>\n",
       "      <td>197</td>\n",
       "      <td>80</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.821283</td>\n",
       "      <td>0.863717</td>\n",
       "      <td>0.733491</td>\n",
       "      <td>0.837993</td>\n",
       "      <td>0.736677</td>\n",
       "      <td>0.967711</td>\n",
       "      <td>0.890152</td>\n",
       "      <td>1019</td>\n",
       "      <td>235</td>\n",
       "      <td>445</td>\n",
       "      <td>76</td>\n",
       "      <td>197</td>\n",
       "      <td>84</td>\n",
       "      <td>34</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954043</td>\n",
       "      <td>0.853282</td>\n",
       "      <td>0.867847</td>\n",
       "      <td>0.747642</td>\n",
       "      <td>0.849916</td>\n",
       "      <td>0.758278</td>\n",
       "      <td>0.956357</td>\n",
       "      <td>0.870722</td>\n",
       "      <td>1008</td>\n",
       "      <td>229</td>\n",
       "      <td>463</td>\n",
       "      <td>88</td>\n",
       "      <td>178</td>\n",
       "      <td>73</td>\n",
       "      <td>46</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.957120</td>\n",
       "      <td>0.849444</td>\n",
       "      <td>0.861357</td>\n",
       "      <td>0.761792</td>\n",
       "      <td>0.840399</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.959203</td>\n",
       "      <td>0.882129</td>\n",
       "      <td>1011</td>\n",
       "      <td>232</td>\n",
       "      <td>449</td>\n",
       "      <td>91</td>\n",
       "      <td>192</td>\n",
       "      <td>70</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.954900</td>\n",
       "      <td>0.835860</td>\n",
       "      <td>0.869104</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.963947</td>\n",
       "      <td>0.912548</td>\n",
       "      <td>1016</td>\n",
       "      <td>240</td>\n",
       "      <td>458</td>\n",
       "      <td>84</td>\n",
       "      <td>184</td>\n",
       "      <td>76</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0</td>\n",
       "      <td>0.617410</td>\n",
       "      <td>0.605043</td>\n",
       "      <td>0.576991</td>\n",
       "      <td>0.551887</td>\n",
       "      <td>0.711587</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.536562</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>565</td>\n",
       "      <td>128</td>\n",
       "      <td>413</td>\n",
       "      <td>106</td>\n",
       "      <td>229</td>\n",
       "      <td>54</td>\n",
       "      <td>488</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630741</td>\n",
       "      <td>0.606723</td>\n",
       "      <td>0.604130</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.720046</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.593542</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>625</td>\n",
       "      <td>156</td>\n",
       "      <td>399</td>\n",
       "      <td>88</td>\n",
       "      <td>243</td>\n",
       "      <td>72</td>\n",
       "      <td>428</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>2</td>\n",
       "      <td>0.613593</td>\n",
       "      <td>0.635028</td>\n",
       "      <td>0.574041</td>\n",
       "      <td>0.610849</td>\n",
       "      <td>0.701456</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.570342</td>\n",
       "      <td>578</td>\n",
       "      <td>150</td>\n",
       "      <td>395</td>\n",
       "      <td>109</td>\n",
       "      <td>246</td>\n",
       "      <td>52</td>\n",
       "      <td>476</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>3</td>\n",
       "      <td>0.622517</td>\n",
       "      <td>0.600855</td>\n",
       "      <td>0.582301</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.704009</td>\n",
       "      <td>0.729469</td>\n",
       "      <td>0.566414</td>\n",
       "      <td>0.574144</td>\n",
       "      <td>597</td>\n",
       "      <td>151</td>\n",
       "      <td>390</td>\n",
       "      <td>105</td>\n",
       "      <td>251</td>\n",
       "      <td>56</td>\n",
       "      <td>457</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>4</td>\n",
       "      <td>0.613694</td>\n",
       "      <td>0.622624</td>\n",
       "      <td>0.580778</td>\n",
       "      <td>0.548463</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.539848</td>\n",
       "      <td>0.509506</td>\n",
       "      <td>569</td>\n",
       "      <td>134</td>\n",
       "      <td>416</td>\n",
       "      <td>98</td>\n",
       "      <td>226</td>\n",
       "      <td>62</td>\n",
       "      <td>485</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928125</td>\n",
       "      <td>0.998230</td>\n",
       "      <td>0.839623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862963</td>\n",
       "      <td>0.997151</td>\n",
       "      <td>0.882576</td>\n",
       "      <td>1050</td>\n",
       "      <td>233</td>\n",
       "      <td>642</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928705</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.860849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>0.871212</td>\n",
       "      <td>1052</td>\n",
       "      <td>230</td>\n",
       "      <td>642</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931063</td>\n",
       "      <td>0.998820</td>\n",
       "      <td>0.863208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895753</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.882129</td>\n",
       "      <td>1052</td>\n",
       "      <td>232</td>\n",
       "      <td>641</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922726</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.841981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>0.855513</td>\n",
       "      <td>1053</td>\n",
       "      <td>225</td>\n",
       "      <td>641</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934042</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>0.884161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911538</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.901141</td>\n",
       "      <td>1052</td>\n",
       "      <td>237</td>\n",
       "      <td>642</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.681747</td>\n",
       "      <td>0.619188</td>\n",
       "      <td>0.661357</td>\n",
       "      <td>0.646226</td>\n",
       "      <td>0.669258</td>\n",
       "      <td>0.661932</td>\n",
       "      <td>0.899335</td>\n",
       "      <td>0.882576</td>\n",
       "      <td>947</td>\n",
       "      <td>233</td>\n",
       "      <td>174</td>\n",
       "      <td>41</td>\n",
       "      <td>468</td>\n",
       "      <td>119</td>\n",
       "      <td>106</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.728150</td>\n",
       "      <td>0.669626</td>\n",
       "      <td>0.683186</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.661111</td>\n",
       "      <td>0.934473</td>\n",
       "      <td>0.901515</td>\n",
       "      <td>984</td>\n",
       "      <td>238</td>\n",
       "      <td>174</td>\n",
       "      <td>38</td>\n",
       "      <td>468</td>\n",
       "      <td>122</td>\n",
       "      <td>69</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.700046</td>\n",
       "      <td>0.670725</td>\n",
       "      <td>0.673746</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.670623</td>\n",
       "      <td>0.899431</td>\n",
       "      <td>0.859316</td>\n",
       "      <td>948</td>\n",
       "      <td>226</td>\n",
       "      <td>194</td>\n",
       "      <td>50</td>\n",
       "      <td>447</td>\n",
       "      <td>111</td>\n",
       "      <td>106</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.682251</td>\n",
       "      <td>0.637130</td>\n",
       "      <td>0.665487</td>\n",
       "      <td>0.648585</td>\n",
       "      <td>0.683774</td>\n",
       "      <td>0.678125</td>\n",
       "      <td>0.859583</td>\n",
       "      <td>0.825095</td>\n",
       "      <td>906</td>\n",
       "      <td>217</td>\n",
       "      <td>222</td>\n",
       "      <td>58</td>\n",
       "      <td>419</td>\n",
       "      <td>103</td>\n",
       "      <td>148</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.692671</td>\n",
       "      <td>0.678767</td>\n",
       "      <td>0.673939</td>\n",
       "      <td>0.661939</td>\n",
       "      <td>0.676284</td>\n",
       "      <td>0.663934</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.923954</td>\n",
       "      <td>961</td>\n",
       "      <td>243</td>\n",
       "      <td>182</td>\n",
       "      <td>37</td>\n",
       "      <td>460</td>\n",
       "      <td>123</td>\n",
       "      <td>93</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983001</td>\n",
       "      <td>0.898532</td>\n",
       "      <td>0.935103</td>\n",
       "      <td>0.813679</td>\n",
       "      <td>0.921358</td>\n",
       "      <td>0.801303</td>\n",
       "      <td>0.979107</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>1031</td>\n",
       "      <td>246</td>\n",
       "      <td>554</td>\n",
       "      <td>99</td>\n",
       "      <td>88</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983742</td>\n",
       "      <td>0.887950</td>\n",
       "      <td>0.928024</td>\n",
       "      <td>0.818396</td>\n",
       "      <td>0.909411</td>\n",
       "      <td>0.819113</td>\n",
       "      <td>0.981956</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1034</td>\n",
       "      <td>240</td>\n",
       "      <td>539</td>\n",
       "      <td>107</td>\n",
       "      <td>103</td>\n",
       "      <td>53</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.982711</td>\n",
       "      <td>0.887939</td>\n",
       "      <td>0.924484</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.909735</td>\n",
       "      <td>0.798635</td>\n",
       "      <td>0.975332</td>\n",
       "      <td>0.889734</td>\n",
       "      <td>1028</td>\n",
       "      <td>234</td>\n",
       "      <td>539</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>59</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.985780</td>\n",
       "      <td>0.881326</td>\n",
       "      <td>0.935103</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.919929</td>\n",
       "      <td>0.812287</td>\n",
       "      <td>0.981025</td>\n",
       "      <td>0.904943</td>\n",
       "      <td>1034</td>\n",
       "      <td>238</td>\n",
       "      <td>551</td>\n",
       "      <td>106</td>\n",
       "      <td>90</td>\n",
       "      <td>55</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.985040</td>\n",
       "      <td>0.881298</td>\n",
       "      <td>0.927476</td>\n",
       "      <td>0.799054</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.976281</td>\n",
       "      <td>0.908745</td>\n",
       "      <td>1029</td>\n",
       "      <td>239</td>\n",
       "      <td>544</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  \\\n",
       "0         Logistic_regression   \n",
       "1         Logistic_regression   \n",
       "2         Logistic_regression   \n",
       "3         Logistic_regression   \n",
       "4         Logistic_regression   \n",
       "5               Random_forest   \n",
       "6               Random_forest   \n",
       "7               Random_forest   \n",
       "8               Random_forest   \n",
       "9               Random_forest   \n",
       "10  Support_vector_classifier   \n",
       "11  Support_vector_classifier   \n",
       "12  Support_vector_classifier   \n",
       "13  Support_vector_classifier   \n",
       "14  Support_vector_classifier   \n",
       "15                    XGBoost   \n",
       "16                    XGBoost   \n",
       "17                    XGBoost   \n",
       "18                    XGBoost   \n",
       "19                    XGBoost   \n",
       "20        K_nearest_neighbors   \n",
       "21        K_nearest_neighbors   \n",
       "22        K_nearest_neighbors   \n",
       "23        K_nearest_neighbors   \n",
       "24        K_nearest_neighbors   \n",
       "25          Gradient_boosting   \n",
       "26          Gradient_boosting   \n",
       "27          Gradient_boosting   \n",
       "28          Gradient_boosting   \n",
       "29          Gradient_boosting   \n",
       "30                Naive_bayes   \n",
       "31                Naive_bayes   \n",
       "32                Naive_bayes   \n",
       "33                Naive_bayes   \n",
       "34                Naive_bayes   \n",
       "35     Balanced_random_forest   \n",
       "36     Balanced_random_forest   \n",
       "37     Balanced_random_forest   \n",
       "38     Balanced_random_forest   \n",
       "39     Balanced_random_forest   \n",
       "40                   AdaBoost   \n",
       "41                   AdaBoost   \n",
       "42                   AdaBoost   \n",
       "43                   AdaBoost   \n",
       "44                   AdaBoost   \n",
       "45                   CatBoost   \n",
       "46                   CatBoost   \n",
       "47                   CatBoost   \n",
       "48                   CatBoost   \n",
       "49                   CatBoost   \n",
       "\n",
       "                                   classifier_details  fold  train_auc  \\\n",
       "0                 LogisticRegression(max_iter=100000)     0   0.674817   \n",
       "1                 LogisticRegression(max_iter=100000)     1   0.657506   \n",
       "2                 LogisticRegression(max_iter=100000)     2   0.665182   \n",
       "3                 LogisticRegression(max_iter=100000)     3   0.664156   \n",
       "4                 LogisticRegression(max_iter=100000)     4   0.671178   \n",
       "5   (DecisionTreeClassifier(max_features='sqrt', m...     0   0.975755   \n",
       "6   (DecisionTreeClassifier(max_features='sqrt', m...     1   0.980471   \n",
       "7   (DecisionTreeClassifier(max_features='sqrt', m...     2   0.975421   \n",
       "8   (DecisionTreeClassifier(max_features='sqrt', m...     3   0.976812   \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', m...     4   0.979435   \n",
       "10                              SVC(probability=True)     0   0.703428   \n",
       "11                              SVC(probability=True)     1   0.695433   \n",
       "12                              SVC(probability=True)     2   0.700718   \n",
       "13                              SVC(probability=True)     3   0.697995   \n",
       "14                              SVC(probability=True)     4   0.705160   \n",
       "15  XGBClassifier(base_score=None, booster=None, c...     0   1.000000   \n",
       "16  XGBClassifier(base_score=None, booster=None, c...     1   1.000000   \n",
       "17  XGBClassifier(base_score=None, booster=None, c...     2   1.000000   \n",
       "18  XGBClassifier(base_score=None, booster=None, c...     3   1.000000   \n",
       "19  XGBClassifier(base_score=None, booster=None, c...     4   1.000000   \n",
       "20                             KNeighborsClassifier()     0   0.907910   \n",
       "21                             KNeighborsClassifier()     1   0.911921   \n",
       "22                             KNeighborsClassifier()     2   0.910367   \n",
       "23                             KNeighborsClassifier()     3   0.901063   \n",
       "24                             KNeighborsClassifier()     4   0.904422   \n",
       "25  ([DecisionTreeRegressor(criterion='friedman_ms...     0   0.955040   \n",
       "26  ([DecisionTreeRegressor(criterion='friedman_ms...     1   0.954853   \n",
       "27  ([DecisionTreeRegressor(criterion='friedman_ms...     2   0.954043   \n",
       "28  ([DecisionTreeRegressor(criterion='friedman_ms...     3   0.957120   \n",
       "29  ([DecisionTreeRegressor(criterion='friedman_ms...     4   0.954900   \n",
       "30                                       GaussianNB()     0   0.617410   \n",
       "31                                       GaussianNB()     1   0.630741   \n",
       "32                                       GaussianNB()     2   0.613593   \n",
       "33                                       GaussianNB()     3   0.622517   \n",
       "34                                       GaussianNB()     4   0.613694   \n",
       "35  (DecisionTreeClassifier(max_features='sqrt', r...     0   1.000000   \n",
       "36  (DecisionTreeClassifier(max_features='sqrt', r...     1   1.000000   \n",
       "37  (DecisionTreeClassifier(max_features='sqrt', r...     2   1.000000   \n",
       "38  (DecisionTreeClassifier(max_features='sqrt', r...     3   1.000000   \n",
       "39  (DecisionTreeClassifier(max_features='sqrt', r...     4   1.000000   \n",
       "40  (DecisionTreeClassifier(max_depth=1, random_st...     0   0.681747   \n",
       "41  (DecisionTreeClassifier(max_depth=1, random_st...     1   0.728150   \n",
       "42  (DecisionTreeClassifier(max_depth=1, random_st...     2   0.700046   \n",
       "43  (DecisionTreeClassifier(max_depth=1, random_st...     3   0.682251   \n",
       "44  (DecisionTreeClassifier(max_depth=1, random_st...     4   0.692671   \n",
       "45  <catboost.core.CatBoostClassifier object at 0x...     0   0.983001   \n",
       "46  <catboost.core.CatBoostClassifier object at 0x...     1   0.983742   \n",
       "47  <catboost.core.CatBoostClassifier object at 0x...     2   0.982711   \n",
       "48  <catboost.core.CatBoostClassifier object at 0x...     3   0.985780   \n",
       "49  <catboost.core.CatBoostClassifier object at 0x...     4   0.985040   \n",
       "\n",
       "    test_auc  train_accuracy  test_accuracy  train_precision  test_precision  \\\n",
       "0   0.609328        0.645428       0.636792         0.660511        0.654494   \n",
       "1   0.676420        0.634218       0.658019         0.651293        0.666667   \n",
       "2   0.651985        0.638938       0.636792         0.655197        0.654391   \n",
       "3   0.650261        0.632448       0.632075         0.649965        0.652422   \n",
       "4   0.641778        0.640920       0.612293         0.657913        0.632708   \n",
       "5   0.868703        0.919764       0.783019         0.904678        0.770440   \n",
       "6   0.880705        0.912684       0.780660         0.893821        0.774920   \n",
       "7   0.873297        0.899115       0.761792         0.882251        0.761290   \n",
       "8   0.870297        0.918584       0.804245         0.909660        0.808219   \n",
       "9   0.886217        0.908019       0.773050         0.891115        0.763407   \n",
       "10  0.634197        0.638938       0.643868         0.636533        0.639506   \n",
       "11  0.697455        0.644248       0.622642         0.638718        0.627451   \n",
       "12  0.676263        0.643068       0.636792         0.640752        0.636591   \n",
       "13  0.675318        0.633038       0.636792         0.630751        0.633252   \n",
       "14  0.632260        0.643868       0.612293         0.640100        0.621622   \n",
       "15  0.927083        1.000000       0.841981         1.000000        0.843206   \n",
       "16  0.917377        1.000000       0.837264         1.000000        0.851986   \n",
       "17  0.920648        1.000000       0.875000         1.000000        0.877698   \n",
       "18  0.917011        1.000000       0.865566         1.000000        0.899225   \n",
       "19  0.903279        1.000000       0.862884         1.000000        0.878229   \n",
       "20  0.777012        0.836578       0.752358         0.842756        0.780919   \n",
       "21  0.741004        0.850147       0.693396         0.853853        0.726351   \n",
       "22  0.747077        0.840708       0.702830         0.846903        0.747292   \n",
       "23  0.774437        0.830678       0.735849         0.837885        0.754209   \n",
       "24  0.763569        0.839623       0.716312         0.840592        0.744027   \n",
       "25  0.825272        0.864307       0.745283         0.838127        0.746835   \n",
       "26  0.821283        0.863717       0.733491         0.837993        0.736677   \n",
       "27  0.853282        0.867847       0.747642         0.849916        0.758278   \n",
       "28  0.849444        0.861357       0.761792         0.840399        0.768212   \n",
       "29  0.835860        0.869104       0.765957         0.846667        0.759494   \n",
       "30  0.605043        0.576991       0.551887         0.711587        0.703297   \n",
       "31  0.606723        0.604130       0.575472         0.720046        0.684211   \n",
       "32  0.635028        0.574041       0.610849         0.701456        0.742574   \n",
       "33  0.600855        0.582301       0.603774         0.704009        0.729469   \n",
       "34  0.622624        0.580778       0.548463         0.715723        0.683673   \n",
       "35  0.928125        0.998230       0.839623         1.000000        0.862963   \n",
       "36  0.928705        0.999410       0.860849         1.000000        0.901961   \n",
       "37  0.931063        0.998820       0.863208         1.000000        0.895753   \n",
       "38  0.922726        0.999410       0.841981         1.000000        0.885827   \n",
       "39  0.934042        0.998821       0.884161         1.000000        0.911538   \n",
       "40  0.619188        0.661357       0.646226         0.669258        0.661932   \n",
       "41  0.669626        0.683186       0.650943         0.677686        0.661111   \n",
       "42  0.670725        0.673746       0.650943         0.679570        0.670623   \n",
       "43  0.637130        0.665487       0.648585         0.683774        0.678125   \n",
       "44  0.678767        0.673939       0.661939         0.676284        0.663934   \n",
       "45  0.898532        0.935103       0.813679         0.921358        0.801303   \n",
       "46  0.887950        0.928024       0.818396         0.909411        0.819113   \n",
       "47  0.887939        0.924484       0.792453         0.909735        0.798635   \n",
       "48  0.881326        0.935103       0.811321         0.919929        0.812287   \n",
       "49  0.881298        0.927476       0.799054         0.913043        0.796667   \n",
       "\n",
       "    train_recall  test_recall  train_true_positives  test_true_positives  \\\n",
       "0       0.883191     0.882576                   930                  233   \n",
       "1       0.885090     0.901515                   932                  238   \n",
       "2       0.885199     0.878327                   933                  231   \n",
       "3       0.886148     0.870722                   934                  229   \n",
       "4       0.879507     0.897338                   927                  236   \n",
       "5       0.973409     0.928030                  1025                  245   \n",
       "6       0.975309     0.912879                  1027                  241   \n",
       "7       0.966793     0.897338                  1019                  236   \n",
       "8       0.964896     0.897338                  1017                  236   \n",
       "9       0.970588     0.920152                  1023                  242   \n",
       "10      0.976258     0.981061                  1028                  259   \n",
       "11      0.983856     0.969697                  1036                  256   \n",
       "12      0.969639     0.965779                  1022                  254   \n",
       "13      0.988615     0.984791                  1042                  259   \n",
       "14      0.975332     0.961977                  1028                  253   \n",
       "15      1.000000     0.916667                  1053                  242   \n",
       "16      1.000000     0.893939                  1053                  236   \n",
       "17      1.000000     0.927757                  1054                  244   \n",
       "18      1.000000     0.882129                  1054                  232   \n",
       "19      1.000000     0.904943                  1054                  238   \n",
       "20      0.905983     0.837121                   954                  221   \n",
       "21      0.915480     0.814394                   964                  215   \n",
       "22      0.907970     0.787072                   957                  207   \n",
       "23      0.902277     0.851711                   951                  224   \n",
       "24      0.915560     0.828897                   965                  218   \n",
       "25      0.968661     0.893939                  1020                  236   \n",
       "26      0.967711     0.890152                  1019                  235   \n",
       "27      0.956357     0.870722                  1008                  229   \n",
       "28      0.959203     0.882129                  1011                  232   \n",
       "29      0.963947     0.912548                  1016                  240   \n",
       "30      0.536562     0.484848                   565                  128   \n",
       "31      0.593542     0.590909                   625                  156   \n",
       "32      0.548387     0.570342                   578                  150   \n",
       "33      0.566414     0.574144                   597                  151   \n",
       "34      0.539848     0.509506                   569                  134   \n",
       "35      0.997151     0.882576                  1050                  233   \n",
       "36      0.999050     0.871212                  1052                  230   \n",
       "37      0.998102     0.882129                  1052                  232   \n",
       "38      0.999051     0.855513                  1053                  225   \n",
       "39      0.998102     0.901141                  1052                  237   \n",
       "40      0.899335     0.882576                   947                  233   \n",
       "41      0.934473     0.901515                   984                  238   \n",
       "42      0.899431     0.859316                   948                  226   \n",
       "43      0.859583     0.825095                   906                  217   \n",
       "44      0.911765     0.923954                   961                  243   \n",
       "45      0.979107     0.931818                  1031                  246   \n",
       "46      0.981956     0.909091                  1034                  240   \n",
       "47      0.975332     0.889734                  1028                  234   \n",
       "48      0.981025     0.904943                  1034                  238   \n",
       "49      0.976281     0.908745                  1029                  239   \n",
       "\n",
       "    train_true_negatives  test_true_negatives  train_false_positives  \\\n",
       "0                    164                   37                    478   \n",
       "1                    143                   41                    499   \n",
       "2                    150                   39                    491   \n",
       "3                    138                   39                    503   \n",
       "4                    160                   23                    482   \n",
       "5                    534                   87                    108   \n",
       "6                    520                   90                    122   \n",
       "7                    505                   87                    136   \n",
       "8                    540                  105                    101   \n",
       "9                    517                   85                    125   \n",
       "10                    55                   14                    587   \n",
       "11                    56                    8                    586   \n",
       "12                    68                   16                    573   \n",
       "13                    31                   11                    610   \n",
       "14                    64                    6                    578   \n",
       "15                   642                  115                      0   \n",
       "16                   642                  119                      0   \n",
       "17                   641                  127                      0   \n",
       "18                   641                  135                      0   \n",
       "19                   642                  127                      0   \n",
       "20                   464                   98                    178   \n",
       "21                   477                   79                    165   \n",
       "22                   468                   91                    173   \n",
       "23                   457                   88                    184   \n",
       "24                   459                   85                    183   \n",
       "25                   445                   80                    197   \n",
       "26                   445                   76                    197   \n",
       "27                   463                   88                    178   \n",
       "28                   449                   91                    192   \n",
       "29                   458                   84                    184   \n",
       "30                   413                  106                    229   \n",
       "31                   399                   88                    243   \n",
       "32                   395                  109                    246   \n",
       "33                   390                  105                    251   \n",
       "34                   416                   98                    226   \n",
       "35                   642                  123                      0   \n",
       "36                   642                  135                      0   \n",
       "37                   641                  134                      0   \n",
       "38                   641                  132                      0   \n",
       "39                   642                  137                      0   \n",
       "40                   174                   41                    468   \n",
       "41                   174                   38                    468   \n",
       "42                   194                   50                    447   \n",
       "43                   222                   58                    419   \n",
       "44                   182                   37                    460   \n",
       "45                   554                   99                     88   \n",
       "46                   539                  107                    103   \n",
       "47                   539                  102                    102   \n",
       "48                   551                  106                     90   \n",
       "49                   544                   99                     98   \n",
       "\n",
       "    test_false_positives  train_false_negatives  test_false_negatives  \n",
       "0                    123                    123                    31  \n",
       "1                    119                    121                    26  \n",
       "2                    122                    121                    32  \n",
       "3                    122                    120                    34  \n",
       "4                    137                    127                    27  \n",
       "5                     73                     28                    19  \n",
       "6                     70                     26                    23  \n",
       "7                     74                     35                    27  \n",
       "8                     56                     37                    27  \n",
       "9                     75                     31                    21  \n",
       "10                   146                     25                     5  \n",
       "11                   152                     17                     8  \n",
       "12                   145                     32                     9  \n",
       "13                   150                     12                     4  \n",
       "14                   154                     26                    10  \n",
       "15                    45                      0                    22  \n",
       "16                    41                      0                    28  \n",
       "17                    34                      0                    19  \n",
       "18                    26                      0                    31  \n",
       "19                    33                      0                    25  \n",
       "20                    62                     99                    43  \n",
       "21                    81                     89                    49  \n",
       "22                    70                     97                    56  \n",
       "23                    73                    103                    39  \n",
       "24                    75                     89                    45  \n",
       "25                    80                     33                    28  \n",
       "26                    84                     34                    29  \n",
       "27                    73                     46                    34  \n",
       "28                    70                     43                    31  \n",
       "29                    76                     38                    23  \n",
       "30                    54                    488                   136  \n",
       "31                    72                    428                   108  \n",
       "32                    52                    476                   113  \n",
       "33                    56                    457                   112  \n",
       "34                    62                    485                   129  \n",
       "35                    37                      3                    31  \n",
       "36                    25                      1                    34  \n",
       "37                    27                      2                    31  \n",
       "38                    29                      1                    38  \n",
       "39                    23                      2                    26  \n",
       "40                   119                    106                    31  \n",
       "41                   122                     69                    26  \n",
       "42                   111                    106                    37  \n",
       "43                   103                    148                    46  \n",
       "44                   123                     93                    20  \n",
       "45                    61                     22                    18  \n",
       "46                    53                     19                    24  \n",
       "47                    59                     26                    29  \n",
       "48                    55                     20                    25  \n",
       "49                    61                     25                    24  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary_output\n",
    "detailed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### - Testing functions HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a champion model (original): \n",
    "\n",
    "def select_champion_model(models: pd.DataFrame, parameters: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the champion model based on the highest or lowest value of a given optimization target.\n",
    "    \n",
    "    Args:\n",
    "    - models: A DataFrame containing model performance metrics.\n",
    "    - optimization_target: The metric by which to select the champion model (e.g., 'test_precision', 'test_recall', 'test_false_positives').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_model: The row corresponding to the selected champion model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if parameters['optimization_target'] not in models.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target'] }' not found in model metrics. Choose from: {list(models.columns)}\")\n",
    "    \n",
    "    # Determine whether we want the highest or lowest value\n",
    "    # Assuming metrics like 'precision', 'recall', 'accuracy' need the highest, and 'false positives', 'false negatives', etc. need the lowest\n",
    "    if 'precision' in parameters['optimization_target']  or 'recall' in parameters['optimization_target']  or 'accuracy' in parameters['optimization_target'] :\n",
    "        ascending = False  # We want to maximize these metrics\n",
    "   \n",
    "    else:\n",
    "        ascending = True  # For counts (e.g., false positives), we want the minimum value\n",
    "    \n",
    "    # Sort the models based on the optimization target\n",
    "    sorted_models = models.sort_values(by=parameters['optimization_target'] , ascending=ascending)\n",
    "    \n",
    "    # Return the top model (first row after sorting)\n",
    "    champion_model = pd.DataFrame(sorted_models.iloc[0]).reset_index()\n",
    "\n",
    "    champion_model.columns = ['element', 'value']\n",
    "    \n",
    "\n",
    "    return champion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_model = select_champion_model(models = detailed_output, parameters= parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>element</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model</td>\n",
       "      <td>Support_vector_classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classifier_details</td>\n",
       "      <td>SVC(probability=True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_auc</td>\n",
       "      <td>0.698313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_auc</td>\n",
       "      <td>0.676015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.633038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.636792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_precision</td>\n",
       "      <td>0.630751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_precision</td>\n",
       "      <td>0.633252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_recall</td>\n",
       "      <td>0.988615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_recall</td>\n",
       "      <td>0.984791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_true_positives</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_true_positives</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_true_negatives</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test_true_negatives</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_false_positives</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_false_positives</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train_false_negatives</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>test_false_negatives</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  element                      value\n",
       "0                   model  Support_vector_classifier\n",
       "1      classifier_details      SVC(probability=True)\n",
       "2                    fold                          3\n",
       "3               train_auc                   0.698313\n",
       "4                test_auc                   0.676015\n",
       "5          train_accuracy                   0.633038\n",
       "6           test_accuracy                   0.636792\n",
       "7         train_precision                   0.630751\n",
       "8          test_precision                   0.633252\n",
       "9            train_recall                   0.988615\n",
       "10            test_recall                   0.984791\n",
       "11   train_true_positives                       1042\n",
       "12    test_true_positives                        259\n",
       "13   train_true_negatives                         31\n",
       "14    test_true_negatives                         11\n",
       "15  train_false_positives                        610\n",
       "16   test_false_positives                        150\n",
       "17  train_false_negatives                         12\n",
       "18   test_false_negatives                          4"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "champion_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next to-dos:  \n",
    "1.) Add parameters for all Classifiers to the parameters model  \n",
    "TODO: Add in ROC/Precision-recall curve\n",
    "Add SMOTE  \n",
    "Add Train and test class balances\n",
    "2.) Add Select \"n\" best logic to the outputs\n",
    "3.) Add in feature importances and feature selection before modeling run\n",
    "3.) Add in Hypterparameter tuning\n",
    "4.) Run with more positions/equity holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# refactor ML function to dynamically import any SKlearn module:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Logistic_regression classifier\n",
      "Training: Random_forest classifier\n",
      "Training: Support_vector_classifier classifier\n",
      "Training: XGBoost classifier\n",
      "Training: K_nearest_neighbors classifier\n",
      "Training: Gradient_boosting classifier\n",
      "Training: Naive_bayes classifier\n",
      "Training: Balanced_random_forest classifier\n",
      "Training: AdaBoost classifier\n",
      "Training: CatBoost classifier\n",
      "------------------\n",
      "Training evaluation completed\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# test out the new function outputs:\n",
    "\n",
    "detailed_output, summary_output, feat_impt, feat_impt_summary = train_models(X_train = X_train, X_test = X_test, y_train = y_train, y_test= y_test, parameters = parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_true_positives</th>\n",
       "      <th>test_true_positives</th>\n",
       "      <th>train_true_negatives</th>\n",
       "      <th>test_true_negatives</th>\n",
       "      <th>train_false_positives</th>\n",
       "      <th>test_false_positives</th>\n",
       "      <th>train_false_negatives</th>\n",
       "      <th>test_false_negatives</th>\n",
       "      <th>train_positive_rate</th>\n",
       "      <th>test_positive_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.696973</td>\n",
       "      <td>0.655087</td>\n",
       "      <td>0.671543</td>\n",
       "      <td>0.651727</td>\n",
       "      <td>0.677314</td>\n",
       "      <td>0.667145</td>\n",
       "      <td>0.900917</td>\n",
       "      <td>0.878491</td>\n",
       "      <td>4746</td>\n",
       "      <td>1157</td>\n",
       "      <td>946</td>\n",
       "      <td>224</td>\n",
       "      <td>2262</td>\n",
       "      <td>578</td>\n",
       "      <td>522</td>\n",
       "      <td>160</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929390</td>\n",
       "      <td>0.998466</td>\n",
       "      <td>0.860790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890801</td>\n",
       "      <td>0.997532</td>\n",
       "      <td>0.884586</td>\n",
       "      <td>5255</td>\n",
       "      <td>1165</td>\n",
       "      <td>3208</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>13</td>\n",
       "      <td>152</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.984055</td>\n",
       "      <td>0.887409</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.806981</td>\n",
       "      <td>0.914695</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.908866</td>\n",
       "      <td>5156</td>\n",
       "      <td>1197</td>\n",
       "      <td>2727</td>\n",
       "      <td>513</td>\n",
       "      <td>481</td>\n",
       "      <td>289</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>0.955187</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>0.865384</td>\n",
       "      <td>0.751778</td>\n",
       "      <td>0.842758</td>\n",
       "      <td>0.754892</td>\n",
       "      <td>0.963176</td>\n",
       "      <td>0.889898</td>\n",
       "      <td>5074</td>\n",
       "      <td>1172</td>\n",
       "      <td>2261</td>\n",
       "      <td>421</td>\n",
       "      <td>947</td>\n",
       "      <td>381</td>\n",
       "      <td>194</td>\n",
       "      <td>145</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>0.907137</td>\n",
       "      <td>0.760620</td>\n",
       "      <td>0.839547</td>\n",
       "      <td>0.720149</td>\n",
       "      <td>0.844398</td>\n",
       "      <td>0.750560</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.823839</td>\n",
       "      <td>4791</td>\n",
       "      <td>1085</td>\n",
       "      <td>2325</td>\n",
       "      <td>441</td>\n",
       "      <td>883</td>\n",
       "      <td>361</td>\n",
       "      <td>477</td>\n",
       "      <td>232</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>0.666568</td>\n",
       "      <td>0.645954</td>\n",
       "      <td>0.638390</td>\n",
       "      <td>0.635194</td>\n",
       "      <td>0.654976</td>\n",
       "      <td>0.652136</td>\n",
       "      <td>0.883827</td>\n",
       "      <td>0.886096</td>\n",
       "      <td>4656</td>\n",
       "      <td>1167</td>\n",
       "      <td>755</td>\n",
       "      <td>179</td>\n",
       "      <td>2453</td>\n",
       "      <td>623</td>\n",
       "      <td>612</td>\n",
       "      <td>150</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>0.619591</td>\n",
       "      <td>0.614055</td>\n",
       "      <td>0.583648</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.710564</td>\n",
       "      <td>0.708645</td>\n",
       "      <td>0.556951</td>\n",
       "      <td>0.545950</td>\n",
       "      <td>2934</td>\n",
       "      <td>719</td>\n",
       "      <td>2013</td>\n",
       "      <td>506</td>\n",
       "      <td>1195</td>\n",
       "      <td>296</td>\n",
       "      <td>2334</td>\n",
       "      <td>598</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>0.977650</td>\n",
       "      <td>0.876040</td>\n",
       "      <td>0.910217</td>\n",
       "      <td>0.777247</td>\n",
       "      <td>0.893953</td>\n",
       "      <td>0.774567</td>\n",
       "      <td>0.970768</td>\n",
       "      <td>0.905833</td>\n",
       "      <td>5114</td>\n",
       "      <td>1193</td>\n",
       "      <td>2601</td>\n",
       "      <td>454</td>\n",
       "      <td>607</td>\n",
       "      <td>348</td>\n",
       "      <td>154</td>\n",
       "      <td>124</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>0.700598</td>\n",
       "      <td>0.663241</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.630477</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.631684</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.972661</td>\n",
       "      <td>5156</td>\n",
       "      <td>1281</td>\n",
       "      <td>274</td>\n",
       "      <td>55</td>\n",
       "      <td>2934</td>\n",
       "      <td>747</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>5268</td>\n",
       "      <td>1192</td>\n",
       "      <td>3208</td>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  train_auc  test_auc  train_accuracy  \\\n",
       "0                   AdaBoost   0.696973  0.655087        0.671543   \n",
       "1     Balanced_random_forest   1.000000  0.929390        0.998466   \n",
       "2                   CatBoost   0.984055  0.887409        0.930038   \n",
       "3          Gradient_boosting   0.955187  0.836820        0.865384   \n",
       "4        K_nearest_neighbors   0.907137  0.760620        0.839547   \n",
       "5        Logistic_regression   0.666568  0.645954        0.638390   \n",
       "6                Naive_bayes   0.619591  0.614055        0.583648   \n",
       "7              Random_forest   0.977650  0.876040        0.910217   \n",
       "8  Support_vector_classifier   0.700598  0.663241        0.640632   \n",
       "9                    XGBoost   1.000000  0.917080        1.000000   \n",
       "\n",
       "   test_accuracy  train_precision  test_precision  train_recall  test_recall  \\\n",
       "0       0.651727         0.677314        0.667145      0.900917     0.878491   \n",
       "1       0.860790         1.000000        0.890801      0.997532     0.884586   \n",
       "2       0.806981         0.914695        0.805601      0.978740     0.908866   \n",
       "3       0.751778         0.842758        0.754892      0.963176     0.889898   \n",
       "4       0.720149         0.844398        0.750560      0.909454     0.823839   \n",
       "5       0.635194         0.654976        0.652136      0.883827     0.886096   \n",
       "6       0.578089         0.710564        0.708645      0.556951     0.545950   \n",
       "7       0.777247         0.893953        0.774567      0.970768     0.905833   \n",
       "8       0.630477         0.637371        0.631684      0.978740     0.972661   \n",
       "9       0.856539         1.000000        0.870069      1.000000     0.905087   \n",
       "\n",
       "   train_true_positives  test_true_positives  train_true_negatives  \\\n",
       "0                  4746                 1157                   946   \n",
       "1                  5255                 1165                  3208   \n",
       "2                  5156                 1197                  2727   \n",
       "3                  5074                 1172                  2261   \n",
       "4                  4791                 1085                  2325   \n",
       "5                  4656                 1167                   755   \n",
       "6                  2934                  719                  2013   \n",
       "7                  5114                 1193                  2601   \n",
       "8                  5156                 1281                   274   \n",
       "9                  5268                 1192                  3208   \n",
       "\n",
       "   test_true_negatives  train_false_positives  test_false_positives  \\\n",
       "0                  224                   2262                   578   \n",
       "1                  659                      0                   143   \n",
       "2                  513                    481                   289   \n",
       "3                  421                    947                   381   \n",
       "4                  441                    883                   361   \n",
       "5                  179                   2453                   623   \n",
       "6                  506                   1195                   296   \n",
       "7                  454                    607                   348   \n",
       "8                   55                   2934                   747   \n",
       "9                  623                      0                   179   \n",
       "\n",
       "   train_false_negatives  test_false_negatives  train_positive_rate  \\\n",
       "0                    522                   160              0.62152   \n",
       "1                     13                   152              0.62152   \n",
       "2                    112                   120              0.62152   \n",
       "3                    194                   145              0.62152   \n",
       "4                    477                   232              0.62152   \n",
       "5                    612                   150              0.62152   \n",
       "6                   2334                   598              0.62152   \n",
       "7                    154                   124              0.62152   \n",
       "8                    112                    36              0.62152   \n",
       "9                      0                   125              0.62152   \n",
       "\n",
       "   test_positive_rate  \n",
       "0             0.62152  \n",
       "1             0.62152  \n",
       "2             0.62152  \n",
       "3             0.62152  \n",
       "4             0.62152  \n",
       "5             0.62152  \n",
       "6             0.62152  \n",
       "7             0.62152  \n",
       "8             0.62152  \n",
       "9             0.62152  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop the ROC/AUC function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def get_roc_auc_score(y_pred: np.array, y_true: np.array, return_plot: bool) -> np.array:\n",
    "\n",
    "    '''Function that returns the auc of the model being run with the functionality to \n",
    "    return the graph output which will be stored in Kedro outputs\n",
    "    \n",
    "        Args:\n",
    "    - y_pred: predictions from the machine learning model\n",
    "    - y_actual: actual values being predicted by the machine learning model\n",
    "    \n",
    "    Returns:\n",
    "    - auc: area under the curve; A higher AUC indicates better classification performance\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # calculate the tpr and fpr for the different threshold values of the classifier outputs: \n",
    "    fpr, tpr, thresholds = roc_curve(y_true = y_true, y_score = y_pred)\n",
    "\n",
    "    # AUC calculation:\n",
    "    auc_output = auc( x = fpr, y = tpr )\n",
    "\n",
    "    if return_plot == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {auc_output:.2f})')\n",
    "        ax.plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--')  # Random guess line\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC Curve' + str(clf))\n",
    "        ax.legend(loc='lower right')\n",
    "\n",
    "        return fig, auc_output\n",
    "    \n",
    "    else:\n",
    "        return auc_output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 640x480 with 1 Axes>, 0.6682393503606587)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict class labels and probabilities\n",
    "y_pred = clf.predict(X_test)               # Predictions (0 or 1)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]         # Probabilities for class 1\n",
    "\n",
    "get_roc_auc_score(y_pred= y_proba, y_true = y_test, return_plot = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modifying the champion model selector to:\n",
    "# pick multiple potential champions for stacking\n",
    "# Pick a minimum threshold for the optimization target\n",
    "# Also solve for the optimization of the difference between train and test split with a min threshold applied\n",
    "\n",
    "\n",
    "def select_champion_model(models: pd.DataFrame, parameters: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the champion model based on the highest or lowest value of a given optimization target.\n",
    "    \n",
    "    Args:\n",
    "    - models: A DataFrame containing model performance metrics.\n",
    "    - optimization_target: The metric by which to select the champion model (e.g., 'test_precision', 'test_recall', 'test_false_positives').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_model: The row corresponding to the selected champion model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if parameters['optimization_target'] not in models.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target'] }' not found in model metrics. Choose from: {list(models.columns)}\")\n",
    "    \n",
    "    # Determine whether we want the highest or lowest value\n",
    "    # Assuming metrics  'precision', 'recall', 'accuracy' need the highest, and 'false positives', 'false negatives', etc. need the lowest\n",
    "    if 'precision' in parameters['optimization_target']  or 'recall' in parameters['optimization_target']  or 'accuracy' in parameters['optimization_target'] :\n",
    "        ascending = False  # We want to maximize these metrics\n",
    "   \n",
    "    else:\n",
    "        ascending = True  # For counts (e.g., false positives), we want the minimum value\n",
    "    \n",
    "\n",
    "    # Sort the models based on the optimization target\n",
    "    sorted_models = models.sort_values(by=parameters['optimization_target'] , ascending=ascending)\n",
    "\n",
    "    # Drop models that do no meet the specified performance threshold:\n",
    "    sorted_models = sorted_models[sorted_models[parameters['optimization_target']] > parameters['optimization_threshold']].reset_index(drop = True)\n",
    "    \n",
    "    # Return the top model (first row after sorting)\n",
    "    champion_model = pd.DataFrame(sorted_models.iloc[0]).reset_index()\n",
    "\n",
    "    champion_model.columns = ['element', 'value']\n",
    "    \n",
    "\n",
    "    return  champion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>element</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model</td>\n",
       "      <td>Support_vector_classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.640632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.630477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_precision</td>\n",
       "      <td>0.637371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_precision</td>\n",
       "      <td>0.631684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_recall</td>\n",
       "      <td>0.97874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_recall</td>\n",
       "      <td>0.972661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_true_positives</td>\n",
       "      <td>5156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_true_positives</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_true_negatives</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_true_negatives</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_false_positives</td>\n",
       "      <td>2934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_false_positives</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_false_negatives</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test_false_negatives</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_positive_rate</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_positive_rate</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  element                      value\n",
       "0                   model  Support_vector_classifier\n",
       "1          train_accuracy                   0.640632\n",
       "2           test_accuracy                   0.630477\n",
       "3         train_precision                   0.637371\n",
       "4          test_precision                   0.631684\n",
       "5            train_recall                    0.97874\n",
       "6             test_recall                   0.972661\n",
       "7    train_true_positives                       5156\n",
       "8     test_true_positives                       1281\n",
       "9    train_true_negatives                        274\n",
       "10    test_true_negatives                         55\n",
       "11  train_false_positives                       2934\n",
       "12   test_false_positives                        747\n",
       "13  train_false_negatives                        112\n",
       "14   test_false_negatives                         36\n",
       "15    train_positive_rate                    0.62152\n",
       "16     test_positive_rate                    0.62152"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_champion_model(models = summary_output, parameters = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_true_positives</th>\n",
       "      <th>test_true_positives</th>\n",
       "      <th>train_true_negatives</th>\n",
       "      <th>test_true_negatives</th>\n",
       "      <th>train_false_positives</th>\n",
       "      <th>test_false_positives</th>\n",
       "      <th>train_false_negatives</th>\n",
       "      <th>test_false_negatives</th>\n",
       "      <th>train_positive_rate</th>\n",
       "      <th>test_positive_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.671543</td>\n",
       "      <td>0.651727</td>\n",
       "      <td>0.677314</td>\n",
       "      <td>0.667145</td>\n",
       "      <td>0.900917</td>\n",
       "      <td>0.878491</td>\n",
       "      <td>4746</td>\n",
       "      <td>1157</td>\n",
       "      <td>946</td>\n",
       "      <td>224</td>\n",
       "      <td>2262</td>\n",
       "      <td>578</td>\n",
       "      <td>522</td>\n",
       "      <td>160</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.860319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892644</td>\n",
       "      <td>0.998291</td>\n",
       "      <td>0.881545</td>\n",
       "      <td>5259</td>\n",
       "      <td>1161</td>\n",
       "      <td>3208</td>\n",
       "      <td>662</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>9</td>\n",
       "      <td>156</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.806981</td>\n",
       "      <td>0.914695</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.908866</td>\n",
       "      <td>5156</td>\n",
       "      <td>1197</td>\n",
       "      <td>2727</td>\n",
       "      <td>513</td>\n",
       "      <td>481</td>\n",
       "      <td>289</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>0.865266</td>\n",
       "      <td>0.752721</td>\n",
       "      <td>0.842620</td>\n",
       "      <td>0.755570</td>\n",
       "      <td>0.963176</td>\n",
       "      <td>0.890656</td>\n",
       "      <td>5074</td>\n",
       "      <td>1173</td>\n",
       "      <td>2260</td>\n",
       "      <td>422</td>\n",
       "      <td>948</td>\n",
       "      <td>380</td>\n",
       "      <td>194</td>\n",
       "      <td>144</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>0.839547</td>\n",
       "      <td>0.720149</td>\n",
       "      <td>0.844398</td>\n",
       "      <td>0.750560</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.823839</td>\n",
       "      <td>4791</td>\n",
       "      <td>1085</td>\n",
       "      <td>2325</td>\n",
       "      <td>441</td>\n",
       "      <td>883</td>\n",
       "      <td>361</td>\n",
       "      <td>477</td>\n",
       "      <td>232</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>0.638390</td>\n",
       "      <td>0.635194</td>\n",
       "      <td>0.654976</td>\n",
       "      <td>0.652136</td>\n",
       "      <td>0.883827</td>\n",
       "      <td>0.886096</td>\n",
       "      <td>4656</td>\n",
       "      <td>1167</td>\n",
       "      <td>755</td>\n",
       "      <td>179</td>\n",
       "      <td>2453</td>\n",
       "      <td>623</td>\n",
       "      <td>612</td>\n",
       "      <td>150</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>0.583648</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.710564</td>\n",
       "      <td>0.708645</td>\n",
       "      <td>0.556951</td>\n",
       "      <td>0.545950</td>\n",
       "      <td>2934</td>\n",
       "      <td>719</td>\n",
       "      <td>2013</td>\n",
       "      <td>506</td>\n",
       "      <td>1195</td>\n",
       "      <td>296</td>\n",
       "      <td>2334</td>\n",
       "      <td>598</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>0.911161</td>\n",
       "      <td>0.773003</td>\n",
       "      <td>0.894287</td>\n",
       "      <td>0.771242</td>\n",
       "      <td>0.972097</td>\n",
       "      <td>0.902809</td>\n",
       "      <td>5121</td>\n",
       "      <td>1189</td>\n",
       "      <td>2602</td>\n",
       "      <td>449</td>\n",
       "      <td>606</td>\n",
       "      <td>353</td>\n",
       "      <td>147</td>\n",
       "      <td>128</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.630477</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.631684</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.972661</td>\n",
       "      <td>5156</td>\n",
       "      <td>1281</td>\n",
       "      <td>274</td>\n",
       "      <td>55</td>\n",
       "      <td>2934</td>\n",
       "      <td>747</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>5268</td>\n",
       "      <td>1192</td>\n",
       "      <td>3208</td>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  train_accuracy  test_accuracy  train_precision  \\\n",
       "0                   AdaBoost        0.671543       0.651727         0.677314   \n",
       "1     Balanced_random_forest        0.998938       0.860319         1.000000   \n",
       "2                   CatBoost        0.930038       0.806981         0.914695   \n",
       "3          Gradient_boosting        0.865266       0.752721         0.842620   \n",
       "4        K_nearest_neighbors        0.839547       0.720149         0.844398   \n",
       "5        Logistic_regression        0.638390       0.635194         0.654976   \n",
       "6                Naive_bayes        0.583648       0.578089         0.710564   \n",
       "7              Random_forest        0.911161       0.773003         0.894287   \n",
       "8  Support_vector_classifier        0.640632       0.630477         0.637371   \n",
       "9                    XGBoost        1.000000       0.856539         1.000000   \n",
       "\n",
       "   test_precision  train_recall  test_recall  train_true_positives  \\\n",
       "0        0.667145      0.900917     0.878491                  4746   \n",
       "1        0.892644      0.998291     0.881545                  5259   \n",
       "2        0.805601      0.978740     0.908866                  5156   \n",
       "3        0.755570      0.963176     0.890656                  5074   \n",
       "4        0.750560      0.909454     0.823839                  4791   \n",
       "5        0.652136      0.883827     0.886096                  4656   \n",
       "6        0.708645      0.556951     0.545950                  2934   \n",
       "7        0.771242      0.972097     0.902809                  5121   \n",
       "8        0.631684      0.978740     0.972661                  5156   \n",
       "9        0.870069      1.000000     0.905087                  5268   \n",
       "\n",
       "   test_true_positives  train_true_negatives  test_true_negatives  \\\n",
       "0                 1157                   946                  224   \n",
       "1                 1161                  3208                  662   \n",
       "2                 1197                  2727                  513   \n",
       "3                 1173                  2260                  422   \n",
       "4                 1085                  2325                  441   \n",
       "5                 1167                   755                  179   \n",
       "6                  719                  2013                  506   \n",
       "7                 1189                  2602                  449   \n",
       "8                 1281                   274                   55   \n",
       "9                 1192                  3208                  623   \n",
       "\n",
       "   train_false_positives  test_false_positives  train_false_negatives  \\\n",
       "0                   2262                   578                    522   \n",
       "1                      0                   140                      9   \n",
       "2                    481                   289                    112   \n",
       "3                    948                   380                    194   \n",
       "4                    883                   361                    477   \n",
       "5                   2453                   623                    612   \n",
       "6                   1195                   296                   2334   \n",
       "7                    606                   353                    147   \n",
       "8                   2934                   747                    112   \n",
       "9                      0                   179                      0   \n",
       "\n",
       "   test_false_negatives  train_positive_rate  test_positive_rate  \n",
       "0                   160              0.62152             0.62152  \n",
       "1                   156              0.62152             0.62152  \n",
       "2                   120              0.62152             0.62152  \n",
       "3                   144              0.62152             0.62152  \n",
       "4                   232              0.62152             0.62152  \n",
       "5                   150              0.62152             0.62152  \n",
       "6                   598              0.62152             0.62152  \n",
       "7                   128              0.62152             0.62152  \n",
       "8                    36              0.62152             0.62152  \n",
       "9                   125              0.62152             0.62152  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
