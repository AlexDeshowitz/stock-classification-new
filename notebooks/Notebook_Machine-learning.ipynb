{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# visualization tools:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# models:\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# evaluation functions:\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook goal: Setup a basic machine learning framework that cleans data, standardizes features,\n",
    "#  evaluates feature impt, shap values, and a myriad of ML algorithms\n",
    "# TODO: add the day-of-week as a feature\n",
    "# TODO: Add in target date versus historic reference dates\n",
    "# TODO: Add in volume-based feature functionality\n",
    "# TODO: Evaluate standardizing features per stock or one model per stock - may not be enough data realistically\n",
    "# TODO: Check bol-range-pct calculation - only giving zero value\n",
    "# TODO: Add profit point forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions:\n",
    "\n",
    "def clean_stock_data(dataframe: pd.DataFrame) -> pd.DataFrame :\n",
    "\n",
    "    '''removes nulls and in the future will be built out to do any additonal cleaning on the dataframe that is necessary\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing all of the potential features\n",
    "        parameters: \n",
    "            calculation_field: field on which all of the features are built\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataset that is ready to load into a machine learning framework\n",
    "    '''\n",
    "\n",
    "    #TODO: In pipeline write this output to the catalogue\n",
    "    # remove records the preceed the target period to have complete information:\n",
    "    dataframe = dataframe.dropna() \n",
    "    #dataframe = dataframe.reset_index(drop = True) # we won't reset the index for now for traceability back to the date, ticker combination later after training\n",
    "\n",
    "    # set the date as an index to us post-forecasting: This is a bad idea, come back to the concept\n",
    "    #dataframe.set_index(keys = 'date', verify_integrity = False, inplace = True) # verify integrity Fale to allow duplicates**\n",
    "    \n",
    "    # remove fields that will not be used as predictive features (can be hardcoded since dataframe structure will be the same):\n",
    "    dataframe = dataframe.drop(columns = [ 'date', 'high', 'low', 'open', 'volume', 'adj_close'])\n",
    "    \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def identify_fields_to_standardize(dataframe: pd.DataFrame, parameters: Dict) -> np.array :\n",
    "\n",
    "    '''creates a list of the continuous fields to standardize by dimension within the predictive model; NOTE: this is used within the standardizer\n",
    "    \n",
    "    Args:\n",
    "        dataframe: dataframe that contains all of the fields of interest to be used in the calculations\n",
    "        parameters:\n",
    "            continuous_feature_cutoff: ratio of unique values to record count to be used to codify continuous features -> removes records from the standardization process which don't have enough data to standardize (e.g., boolean)\n",
    "\n",
    "    Returns: list of continuous fields to use in the standardization process based on user's specifications of \"uniqueness\" threshold    \n",
    "\n",
    "    '''\n",
    "\n",
    "    numeric_fields = dataframe.select_dtypes(include = 'number').columns\n",
    "    records = len(dataframe)\n",
    "\n",
    "    record_summary = pd.DataFrame(dataframe[numeric_fields].nunique(), columns = ['unique_values'])\n",
    "    record_summary['rows_in_df'] = records\n",
    "    record_summary['value_to_record_ratio'] = record_summary['unique_values']/ record_summary['rows_in_df']\n",
    "\n",
    "    # filter for a threshold specified by the user:\n",
    "    record_summary = record_summary[record_summary['value_to_record_ratio'] > parameters['continuous_feature_cutoff']]\n",
    "\n",
    "    # remove percentage features # TODO: later add in functionality to remove percentage based features\n",
    "\n",
    "    return record_summary.index\n",
    "\n",
    "\n",
    "# Justification for approach on scaling - the argument can be made that since our approach will generalize movemements across multiple securities that we need to standardize each security to its own price range.  Therefore, any features with price-relative values will be scaled per the security's price values to avoid odd splits in tree-based algos\n",
    "# the concern with standardization is generally focused on not letting any one feature have considerably more weight in a model than another; however in this case, \n",
    "\n",
    "\n",
    "def standardize_continuous_features(dataframe: pd.DataFrame, parameters: Dict) -> pd.DataFrame:\n",
    "\n",
    "    '''function that identifies the continuious features in the dataframe and standardizes each feature by equity to enable scaling relative to each equity\n",
    "    \n",
    "    Args:\n",
    "        Dataframe: Pandas dataframe to be used in machine learning\n",
    "        Parameters:\n",
    "            stock_field: field indicating the stock for the window function to scan\n",
    "            calculation_field: field for which the target is being calculated (used for drop in main row merge)\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe: containing the standardized data fields\n",
    "    \n",
    "    '''\n",
    "\n",
    "    continuous_fields = list(identify_fields_to_standardize(dataframe = dataframe, parameters = parameters))\n",
    "\n",
    "    # add in the ticker for grouping next:\n",
    "    continuous_fields.append(parameters['stock_field'])\n",
    "\n",
    "    # downselect to the fields that will be used to standardize:\n",
    "    continuous_dataframe = dataframe[continuous_fields]\n",
    "\n",
    "    # calculate z-scores: --> Standardizes within each feature to scale accordingly\n",
    "    z_scores = (continuous_dataframe - continuous_dataframe.groupby(by = parameters['stock_field']).transform('mean')) / continuous_dataframe.groupby(by = parameters['stock_field']).transform('std')\n",
    "\n",
    "    # drop the null ticker (not needed post groupby): \n",
    "    z_scores.drop(columns = [ parameters['stock_field'], parameters['calculation_field'] ], inplace = True)\n",
    "\n",
    "    # rename the fields to indicate standardization:\n",
    "    z_scores.columns = z_scores.columns + '_standardized'\n",
    "\n",
    "    # drop original continuous fields # TODO: coming back after calculation checks:\n",
    "    if parameters['drop_original_fields'] == True:\n",
    "        continuous_fields.remove(parameters['stock_field'])\n",
    "        dataframe.drop(columns = continuous_fields, inplace = True)\n",
    "\n",
    "    # append the fields back into the core dataframe:\n",
    "    z_scores = pd.concat([dataframe, z_scores], axis = 1)\n",
    "\n",
    "    # remove the standardized target field:\n",
    "    z_scores.drop(columns = z_scores.columns[z_scores.columns.str.contains('target')][1], inplace = True)\n",
    "\n",
    "    # remove unnecessary items:\n",
    "    del continuous_fields, continuous_dataframe\n",
    "\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode_tickers(dataframe: pd.DataFrame, parameters: Dict) -> pd.DataFrame:\n",
    "\n",
    "    '''Returns one-hot encoded features to the predictive dataset NOTE: May not work, but this retains some of the information in the original dataframe while also potentially giving the global model a nudge\n",
    "       Note: we choose not to drop first for now, even though it's a trap; Can be used post processing or as model features\n",
    "    Args:\n",
    "        dataframe: core dataset that has been augmented with additional features\n",
    "        parameters:\n",
    "            stock_field: text field containing the 3 letter ticker of the dataset\n",
    "    Returns:   \n",
    "        dataframe with augmented columns\n",
    "    \n",
    "    '''\n",
    "\n",
    "    dataframe = pd.get_dummies(data = dataframe, prefix = \"ind\", columns = [parameters['stock_field']], drop_first = False)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def profile_target_variable(dataframe: pd.DataFrame, parameters: dict):\n",
    "\n",
    "\n",
    "    '''Function that looks at the target variable and creates an output for the user to review and decide whether rebalancing will help classification task\n",
    "    Args:\n",
    "        dataframe: Main resulting dataframe from all data conversion steps\n",
    "        parameters:\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    # isolate the target variable:\n",
    "    target_field = list(dataframe.columns[dataframe.columns.str.contains('target')])\n",
    "\n",
    "    # create simple value count outputs:\n",
    "    target_summary_table = pd.DataFrame(dataframe[target_field].value_counts()).reset_index()\n",
    "    target_summary_table.rename(columns = {0 : 'counts'}, inplace = True)\n",
    "    target_summary_table['proportion'] = target_summary_table['counts'] / target_summary_table['counts'].sum()\n",
    "\n",
    "    # create bargraph and save it:\n",
    "    ''' TODO : resolve ability to output a matplotlib plot in kedro catalog\n",
    "    sns.countplot(x=target_field, data=dataframe)\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.show() '''\n",
    "    target_field =', '.join(dataframe.columns[dataframe.columns.str.contains('target')].str.replace(r'\\[|\\]', ''))\n",
    "    positive_proportion = target_summary_table[target_summary_table[target_field].astype(int) == 1]['proportion'].to_list()\n",
    "   \n",
    "\n",
    "    print('Classification target: ' + str(target_field) + \" contains a class balance of: \" + str(positive_proportion) + \" in the positive case\")\n",
    "           \n",
    "\n",
    "    return target_summary_table # TODO: Write this to the catalogue as a reporting output for the users\n",
    "\n",
    "\n",
    "\n",
    "def create_training_test_splits(dataframe: pd.DataFrame, parameters: Dict) :\n",
    "\n",
    "    '''Function that splits out training and test sets for machine learning; for the purposes of this model the way we piose the problem allows for random train test split\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing only the target field and the features to be used by the classifier\n",
    "        parameters:\n",
    "            test_ratio: proportion of samples in the dataframe to be used as a test set once the models are tuned and evaluated\n",
    "    \n",
    "    Returns:\n",
    "        X_train: training set for use in ML process\n",
    "        X_test: test set to be held out until all cross-validation is completed\n",
    "        y_train: training set for target variables\n",
    "        y_test: test target to be held out until all cross-validation is completed\n",
    "\n",
    "    '''\n",
    "\n",
    "    # define Y and x:\n",
    "    target_feature = list(dataframe.columns[dataframe.columns.str.contains('target')])\n",
    "\n",
    "    y = dataframe[target_feature]\n",
    "    X = dataframe.drop(columns = target_feature)\n",
    "\n",
    "    # create the training and test splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=parameters['test_size'], random_state=parameters['seed'], stratify = y)\n",
    "\n",
    "    #y_train = y_train.values.ravel()\n",
    "    #y_test = y_test.values.ravel()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def custom_recall_score(confusion_matrix: np.array) -> np.int64 :\n",
    "\n",
    "    recall_value = confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[1,0])\n",
    "\n",
    "    return recall_value\n",
    "\n",
    "\n",
    "def custom_precision_score(confusion_matrix: np.array) -> np.int64 : \n",
    "\n",
    "    precision_value = confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1])\n",
    "\n",
    "    return precision_value\n",
    "\n",
    "\n",
    "def extract_feature_importances(X: np.array, model: BaseEstimator) -> pd.DataFrame:\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract feature importances: \n",
    "    feature_importances = model.feature_importances_\n",
    "    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "                                          'Feature': feature_names,\n",
    "                                          'Importance': feature_importances\n",
    "                                        }\n",
    "                                        ).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "\n",
    "def select_champion_model(models: pd.DataFrame, parameters: str):\n",
    "    \"\"\"\n",
    "    Selects the champion model based on the highest or lowest value of a given optimization target.\n",
    "    \n",
    "    Args:\n",
    "    - models: A DataFrame containing model performance metrics.\n",
    "    - optimization_target: The metric by which to select the champion model (e.g., 'test_precision', 'test_recall', 'test_false_positives').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_model: The row corresponding to the selected champion model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if parameters['optimization_target'] not in models.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target'] }' not found in model metrics. Choose from: {list(models.columns)}\")\n",
    "    \n",
    "    # Determine whether we want the highest or lowest value\n",
    "    # Assuming metrics like 'precision', 'recall', 'accuracy' need the highest, and 'false positives', 'false negatives', etc. need the lowest\n",
    "    if 'precision' in parameters['optimization_target']  or 'recall' in parameters['optimization_target']  or 'accuracy' in parameters['optimization_target'] :\n",
    "        ascending = False  # We want to maximize these metrics\n",
    "   \n",
    "    else:\n",
    "        ascending = True  # For counts (e.g., false positives), we want the minimum value\n",
    "    \n",
    "    # Sort the models based on the optimization target\n",
    "    sorted_models = models.sort_values(by=parameters['optimization_target'] , ascending=ascending)\n",
    "    \n",
    "    # Return the top model (first row after sorting)\n",
    "    champion_model = pd.DataFrame(sorted_models.iloc[0]).reset_index()\n",
    "\n",
    "    champion_model.columns = ['element', 'value']\n",
    "    \n",
    "\n",
    "    return champion_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = catalog.load('combined_modeling_input')\n",
    "df = pd.read_csv('../data/03_primary/combined_modeling_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the parameters for the model: \n",
    "\n",
    "parameters = {'continuous_feature_cutoff' : 0.6,\n",
    "              'stock_field' : 'ticker',\n",
    "              'calculation_field' : 'close',\n",
    "              'drop_original_fields' : True,\n",
    "              'drop_stock_field': True, # keep this fixed \n",
    "              'test_size' : 0.20, # proportion of the dataset held out as the test set\n",
    "              'seed' : 1187,\n",
    "              'cross_val_splits' : 5,\n",
    "              'c' : 1.0,\n",
    "              'kernel' : 'rbf',\n",
    "              'gamma' : 'scale',\n",
    "              'optimization_target' : 'test_recall', # other_useful_inputs: train_accuracy, test_accuracy, train_precision, train_recall, test_recall, true_positives, false_positives\n",
    "              'optimization_threshold' : 0.8,\n",
    "              # specify the algorithms to be used:\n",
    "              'classifiers': {\n",
    "                'Logistic_regression': {\n",
    "                    'class_path': 'sklearn.linear_model.LogisticRegression',\n",
    "                    'params': {'penalty': 'l2', 'C': 1.0, 'max_iter': 100000}\n",
    "                },\n",
    "                'Random_forest': {\n",
    "                    'class_path': 'sklearn.ensemble.RandomForestClassifier',\n",
    "                    'params': {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
    "                },\n",
    "                'Support_vector_classifier': {\n",
    "                    'class_path': 'sklearn.svm.SVC',\n",
    "                    'params': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale', 'probability': True}\n",
    "                },\n",
    "                'XGBoost': {\n",
    "                    'class_path': 'xgboost.XGBClassifier',\n",
    "                    'params': {'use_label_encoder': False, 'eval_metric': 'logloss'}\n",
    "                },\n",
    "                'K_nearest_neighbors': {\n",
    "                    'class_path': 'sklearn.neighbors.KNeighborsClassifier',\n",
    "                    'params': {'n_neighbors': 5, 'weights': 'uniform'}\n",
    "                },\n",
    "                'Gradient_boosting': {\n",
    "                    'class_path': 'sklearn.ensemble.GradientBoostingClassifier',\n",
    "                    'params': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3}\n",
    "                },\n",
    "                'Naive_bayes': {\n",
    "                    'class_path': 'sklearn.naive_bayes.GaussianNB',\n",
    "                    'params': {}\n",
    "                },\n",
    "                'Balanced_random_forest': {\n",
    "                    'class_path': 'imblearn.ensemble.BalancedRandomForestClassifier',\n",
    "                    'params': {'n_estimators': 200, 'max_features': 'sqrt', 'sampling_strategy' : 'all', 'bootstrap' : False, 'replacement' : True}\n",
    "                },\n",
    "                'AdaBoost': {\n",
    "                    'class_path': 'sklearn.ensemble.AdaBoostClassifier',\n",
    "                    'params': {'n_estimators': 50, 'learning_rate': 1.0, 'algorithm' : 'SAMME'}\n",
    "                },\n",
    "                'CatBoost': {\n",
    "                    'class_path': 'catboost.CatBoostClassifier',\n",
    "                    'params': {'iterations': 100, 'learning_rate': 0.1, 'depth': 6, 'silent': True}\n",
    "                }\n",
    "            }\n",
    "\n",
    "}\n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the null values from the dataset and drop un-needed columns for the classifier:\n",
    "test = clean_stock_data(dataframe= df)\n",
    "# test: standardize features:\n",
    "test = standardize_continuous_features(dataframe = test, parameters = parameters)\n",
    "# one-hot encode: \n",
    "test = one_hot_encode_tickers(dataframe = test, parameters= parameters)\n",
    "# create training and test sets\n",
    "X_train, X_test, y_train, y_test = create_training_test_splits(dataframe=test, parameters= parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### - Function development HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### - Testing functions HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a champion model: \n",
    "\n",
    "def select_champion_model(models: pd.DataFrame, parameters: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the champion model based on the highest or lowest value of a given optimization target.\n",
    "    \n",
    "    Args:\n",
    "    - models: A DataFrame containing model performance metrics.\n",
    "    - optimization_target: The metric by which to select the champion model (e.g., 'test_precision', 'test_recall', 'test_false_positives').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_model: The row corresponding to the selected champion model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if parameters['optimization_target'] not in models.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target'] }' not found in model metrics. Choose from: {list(models.columns)}\")\n",
    "    \n",
    "    # Determine whether we want the highest or lowest value\n",
    "    # Assuming metrics like 'precision', 'recall', 'accuracy' need the highest, and 'false positives', 'false negatives', etc. need the lowest\n",
    "    if 'precision' in parameters['optimization_target']  or 'recall' in parameters['optimization_target']  or 'accuracy' in parameters['optimization_target'] :\n",
    "        ascending = False  # We want to maximize these metrics\n",
    "   \n",
    "    else:\n",
    "        ascending = True  # For counts (e.g., false positives), we want the minimum value\n",
    "    \n",
    "    # Sort the models based on the optimization target\n",
    "    sorted_models = models.sort_values(by=parameters['optimization_target'] , ascending=ascending)\n",
    "    \n",
    "    # Return the top model (first row after sorting)\n",
    "    champion_model = pd.DataFrame(sorted_models.iloc[0]).reset_index()\n",
    "\n",
    "    champion_model.columns = ['element', 'value']\n",
    "    \n",
    "\n",
    "    return champion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#champion_model = select_champion_model(models = detailed_output, parameters= parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1    (DecisionTreeClassifier(max_features='sqrt', r...\\nName: value, dtype: object\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#str(champion_model[champion_model['element']=='classifier_details']['value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next to-dos:  \n",
    "1.) Add parameters for all Classifiers to the parameters model  \n",
    "TODO: Add in ROC/Precision-recall curve\n",
    "Add SMOTE  \n",
    "Add Train and test class balances\n",
    "2.) Add Select \"n\" best logic to the outputs\n",
    "3.) Add in feature importances and feature selection before modeling run\n",
    "3.) Add in Hypterparameter tuning\n",
    "4.) Run with more positions/equity holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# refactor ML function to dynamically import any SKlearn module:\n",
    "\n",
    "\n",
    "def dynamic_import(class_path: str):\n",
    "    \"\"\"Dynamically imports a class from its full path.\"\"\"\n",
    "    module_name, class_name = class_path.rsplit('.', 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)\n",
    "\n",
    "def train_models(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series, parameters: dict) -> pd.DataFrame:\n",
    "    \n",
    "    '''WRITE DOCUMENTATION'''\n",
    "    \n",
    "    # Convert y_train to a 1D array if it's a DataFrame\n",
    "    #y_train = y_train.iloc[:, 0].values if isinstance(y_train, pd.DataFrame) else y_train.values\n",
    "    y_train = y_train.values.ravel() if isinstance(y_train, pd.DataFrame) else y_train.ravel()\n",
    "    # Store feature names from the DataFrame\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    model, classifier_details, fold = [], [], []\n",
    "    train_aucs, test_aucs = [], []\n",
    "    train_precisions, test_precisions = [], []\n",
    "    train_recalls, test_recalls = [], []\n",
    "    train_f_scores, test_f_scores = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    train_true_positives, test_true_positives = [], []\n",
    "    train_true_negatives, test_true_negatives = [], []\n",
    "    train_false_positives, test_false_positives = [], []\n",
    "    train_false_negatives, test_false_negatives = [], []\n",
    "    \n",
    "    # Initialize feature importance storage\n",
    "    feature_importances_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through classifiers specified in the parameters\n",
    "    for clf_name, clf_info in parameters['classifiers'].items():\n",
    "        # Dynamically import and instantiate classifier\n",
    "        clf_class = dynamic_import(clf_info['class_path'])\n",
    "        clf_params = clf_info['params']\n",
    "        clf = clf_class(**clf_params)  # Initialize classifier with params\n",
    "\n",
    "        print(f'Training: {clf_name} classifier')\n",
    "\n",
    "        # Cross-validation loop\n",
    "        cv = StratifiedKFold(n_splits=parameters['cross_val_splits'], shuffle=True, random_state=parameters['seed']).split(X_train, y_train)\n",
    "\n",
    "        for k, (fold_train, fold_test) in enumerate(cv):\n",
    "            #clf.fit(X_train.iloc[fold_train], y_train[fold_train])\n",
    "            clf.fit(X_train.iloc[fold_train], y_train[fold_train])  # Keep DataFrame format\n",
    "            train_pred = clf.predict(X_train.iloc[fold_train])           \n",
    "            test_pred = clf.predict(X_train.iloc[fold_test])  \n",
    "            \n",
    "            # Predictions (at the 50% threshold)\n",
    "            train_pred = clf.predict(X_train.iloc[fold_train])\n",
    "            test_pred = clf.predict(X_train.iloc[fold_test])\n",
    "\n",
    "            \n",
    "            # AUC \n",
    "            if hasattr(clf, \"predict_proba\"): # Note: error, means predict proba exists, but must be set to true in params\n",
    "                train_pred_proba = clf.predict_proba(X_train.iloc[fold_train])[:, 1]\n",
    "                test_pred_proba = clf.predict_proba(X_train.iloc[fold_test])[:, 1] \n",
    "\n",
    "                train_auc = get_roc_auc_score(y_pred = train_pred_proba, y_true = y_train[fold_train])\n",
    "                test_auc = get_roc_auc_score(y_pred = test_pred_proba, y_true = y_train[fold_test])\n",
    "\n",
    "            else:\n",
    "                train_auc = np.nan\n",
    "                test_auc = np.nan\n",
    "            \n",
    "            \n",
    "            # Confusion matrices\n",
    "            train_confusion_matrix = confusion_matrix(y_train[fold_train], train_pred)\n",
    "            test_confusion_matrix = confusion_matrix(y_train[fold_test], test_pred)\n",
    "\n",
    "            # Accuracy\n",
    "            train_accuracy = clf.score(X_train.iloc[fold_train], y_train[fold_train])\n",
    "            test_accuracy = clf.score(X_train.iloc[fold_test], y_train[fold_test])\n",
    "\n",
    "            # Precision\n",
    "            train_precision = precision_score(y_train[fold_train], train_pred)\n",
    "            test_precision = precision_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # Recall\n",
    "            train_recall = recall_score(y_train[fold_train], train_pred)\n",
    "            test_recall = recall_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # F1 Score\n",
    "            train_f = f1_score(y_train[fold_train], train_pred)\n",
    "            test_f = f1_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # True/False Positives/Negatives\n",
    "            train_tp = train_confusion_matrix[1,1]\n",
    "            test_tp = test_confusion_matrix[1,1]\n",
    "            train_tn = train_confusion_matrix[0,0]\n",
    "            test_tn = test_confusion_matrix[0,0]\n",
    "            train_fp = train_confusion_matrix[0,1]\n",
    "            test_fp = test_confusion_matrix[0,1]\n",
    "            train_fn = train_confusion_matrix[1,0]\n",
    "            test_fn = test_confusion_matrix[1,0]\n",
    "\n",
    "            # Append metrics\n",
    "            model.append(clf_name)\n",
    "            classifier_details.append(clf)\n",
    "            fold.append(k)\n",
    "            train_aucs.append(train_auc)\n",
    "            test_aucs.append(test_auc)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            train_precisions.append(train_precision)\n",
    "            test_precisions.append(test_precision)\n",
    "            train_recalls.append(train_recall)\n",
    "            test_recalls.append(test_recall)\n",
    "            train_f_scores.append(train_f)\n",
    "            test_f_scores.append(test_f)\n",
    "            train_true_positives.append(train_tp)\n",
    "            test_true_positives.append(test_tp)\n",
    "            train_true_negatives.append(train_tn)\n",
    "            test_true_negatives.append(test_tn)\n",
    "            train_false_positives.append(train_fp)\n",
    "            test_false_positives.append(test_fp)\n",
    "            train_false_negatives.append(train_fn)\n",
    "            test_false_negatives.append(test_fn)\n",
    "\n",
    "            # Extract feature importances or coefficients\n",
    "            if hasattr(clf, \"coef_\"):  # For LogisticRegression and other linear models\n",
    "                feature_importances = clf.coef_.flatten()\n",
    "                temp_df = pd.DataFrame({\n",
    "                    \"model\": [clf_name]*len(feature_importances),\n",
    "                    \"fold\": [k]*len(feature_importances),\n",
    "                    \"feature\": feature_names,\n",
    "                    \"importance\": feature_importances\n",
    "                })\n",
    "                feature_importances_df = pd.concat([feature_importances_df, temp_df], ignore_index=True)\n",
    "            \n",
    "            elif hasattr(clf, \"feature_importances_\"):  # For RandomForest, XGBoost, and other ensemble/non-linear models\n",
    "                feature_importances = clf.feature_importances_\n",
    "                temp_df = pd.DataFrame({\n",
    "                    \"model\": [clf_name]*len(feature_importances),\n",
    "                    \"fold\": [k]*len(feature_importances),\n",
    "                    \"feature\": feature_names,\n",
    "                    \"importance\": feature_importances\n",
    "                })\n",
    "                feature_importances_df = pd.concat([feature_importances_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Summary of feature importances\n",
    "    feature_importances_summary_df = feature_importances_df.groupby(['model', 'feature']).agg(\n",
    "        importance_value=('importance', 'mean')).reset_index()\n",
    "\n",
    "    # Detailed results DataFrame\n",
    "    detailed_results_df = pd.DataFrame({\n",
    "        \"model\": model,\n",
    "        \"classifier_details\": classifier_details,\n",
    "        \"fold\": fold,\n",
    "        \"train_auc\" : train_aucs,\n",
    "        \"test_auc\" : test_aucs,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"test_accuracy\": test_accuracies,\n",
    "        \"train_precision\": train_precisions,\n",
    "        \"test_precision\": test_precisions,\n",
    "        \"train_recall\": train_recalls,\n",
    "        \"test_recall\": test_recalls,\n",
    "        \"train_true_positives\": train_true_positives,\n",
    "        \"test_true_positives\": test_true_positives,\n",
    "        \"train_true_negatives\": train_true_negatives,\n",
    "        \"test_true_negatives\": test_true_negatives,\n",
    "        \"train_false_positives\": train_false_positives,\n",
    "        \"test_false_positives\": test_false_positives,\n",
    "        \"train_false_negatives\": train_false_negatives,\n",
    "        \"test_false_negatives\": test_false_negatives\n",
    "    })\n",
    "\n",
    "    # Aggregated results\n",
    "    results_df = detailed_results_df.groupby('model').agg({\n",
    "        'train_auc' : 'mean',\n",
    "        'test_auc' : 'mean',\n",
    "        'train_accuracy': 'mean',\n",
    "        'test_accuracy': 'mean',\n",
    "        'train_precision': 'mean',\n",
    "        'test_precision': 'mean',\n",
    "        'train_recall': 'mean',\n",
    "        'test_recall': 'mean',\n",
    "        'train_true_positives': 'sum',\n",
    "        'test_true_positives': 'sum',\n",
    "        'train_true_negatives': 'sum',\n",
    "        'test_true_negatives': 'sum',\n",
    "        'train_false_positives': 'sum',\n",
    "        'test_false_positives': 'sum',\n",
    "        'train_false_negatives': 'sum',\n",
    "        'test_false_negatives': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Add in overall positve-class rates to the datasets:\n",
    "    results_df['train_positive_rate'] = (results_df['train_true_positives'] + results_df['train_false_negatives']) / (results_df['train_true_positives'] + results_df['train_false_negatives'] + results_df['train_false_positives'] + results_df['train_true_negatives'])\n",
    "    results_df['test_positive_rate'] = (results_df['test_true_positives'] + results_df['test_false_negatives']) / (results_df['test_true_positives'] + results_df['test_false_negatives'] + results_df['test_false_positives'] + results_df['test_true_negatives'])\n",
    "\n",
    "    # share message with complete:\n",
    "    print('------------------')\n",
    "    print('Training evaluation completed')\n",
    "    print('------------------')\n",
    "\n",
    "    return detailed_results_df, results_df, feature_importances_df, feature_importances_summary_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Logistic_regression classifier\n",
      "Training: Random_forest classifier\n",
      "Training: Support_vector_classifier classifier\n",
      "Training: XGBoost classifier\n",
      "Training: K_nearest_neighbors classifier\n",
      "Training: Gradient_boosting classifier\n",
      "Training: Naive_bayes classifier\n",
      "Training: Balanced_random_forest classifier\n",
      "Training: AdaBoost classifier\n",
      "Training: CatBoost classifier\n",
      "------------------\n",
      "Training evaluation completed\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# test out the new function outputs:\n",
    "\n",
    "detailed_output, summary_output, feat_impt, feat_impt_summary = train_models(X_train = X_train, X_test = X_test, y_train = y_train, y_test= y_test, parameters = parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_true_positives</th>\n",
       "      <th>test_true_positives</th>\n",
       "      <th>train_true_negatives</th>\n",
       "      <th>test_true_negatives</th>\n",
       "      <th>train_false_positives</th>\n",
       "      <th>test_false_positives</th>\n",
       "      <th>train_false_negatives</th>\n",
       "      <th>test_false_negatives</th>\n",
       "      <th>train_positive_rate</th>\n",
       "      <th>test_positive_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.696973</td>\n",
       "      <td>0.655087</td>\n",
       "      <td>0.671543</td>\n",
       "      <td>0.651727</td>\n",
       "      <td>0.677314</td>\n",
       "      <td>0.667145</td>\n",
       "      <td>0.900917</td>\n",
       "      <td>0.878491</td>\n",
       "      <td>4746</td>\n",
       "      <td>1157</td>\n",
       "      <td>946</td>\n",
       "      <td>224</td>\n",
       "      <td>2262</td>\n",
       "      <td>578</td>\n",
       "      <td>522</td>\n",
       "      <td>160</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929390</td>\n",
       "      <td>0.998466</td>\n",
       "      <td>0.860790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890801</td>\n",
       "      <td>0.997532</td>\n",
       "      <td>0.884586</td>\n",
       "      <td>5255</td>\n",
       "      <td>1165</td>\n",
       "      <td>3208</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>13</td>\n",
       "      <td>152</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.984055</td>\n",
       "      <td>0.887409</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.806981</td>\n",
       "      <td>0.914695</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.908866</td>\n",
       "      <td>5156</td>\n",
       "      <td>1197</td>\n",
       "      <td>2727</td>\n",
       "      <td>513</td>\n",
       "      <td>481</td>\n",
       "      <td>289</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>0.955187</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>0.865384</td>\n",
       "      <td>0.751778</td>\n",
       "      <td>0.842758</td>\n",
       "      <td>0.754892</td>\n",
       "      <td>0.963176</td>\n",
       "      <td>0.889898</td>\n",
       "      <td>5074</td>\n",
       "      <td>1172</td>\n",
       "      <td>2261</td>\n",
       "      <td>421</td>\n",
       "      <td>947</td>\n",
       "      <td>381</td>\n",
       "      <td>194</td>\n",
       "      <td>145</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>0.907137</td>\n",
       "      <td>0.760620</td>\n",
       "      <td>0.839547</td>\n",
       "      <td>0.720149</td>\n",
       "      <td>0.844398</td>\n",
       "      <td>0.750560</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.823839</td>\n",
       "      <td>4791</td>\n",
       "      <td>1085</td>\n",
       "      <td>2325</td>\n",
       "      <td>441</td>\n",
       "      <td>883</td>\n",
       "      <td>361</td>\n",
       "      <td>477</td>\n",
       "      <td>232</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>0.666568</td>\n",
       "      <td>0.645954</td>\n",
       "      <td>0.638390</td>\n",
       "      <td>0.635194</td>\n",
       "      <td>0.654976</td>\n",
       "      <td>0.652136</td>\n",
       "      <td>0.883827</td>\n",
       "      <td>0.886096</td>\n",
       "      <td>4656</td>\n",
       "      <td>1167</td>\n",
       "      <td>755</td>\n",
       "      <td>179</td>\n",
       "      <td>2453</td>\n",
       "      <td>623</td>\n",
       "      <td>612</td>\n",
       "      <td>150</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>0.619591</td>\n",
       "      <td>0.614055</td>\n",
       "      <td>0.583648</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.710564</td>\n",
       "      <td>0.708645</td>\n",
       "      <td>0.556951</td>\n",
       "      <td>0.545950</td>\n",
       "      <td>2934</td>\n",
       "      <td>719</td>\n",
       "      <td>2013</td>\n",
       "      <td>506</td>\n",
       "      <td>1195</td>\n",
       "      <td>296</td>\n",
       "      <td>2334</td>\n",
       "      <td>598</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>0.977650</td>\n",
       "      <td>0.876040</td>\n",
       "      <td>0.910217</td>\n",
       "      <td>0.777247</td>\n",
       "      <td>0.893953</td>\n",
       "      <td>0.774567</td>\n",
       "      <td>0.970768</td>\n",
       "      <td>0.905833</td>\n",
       "      <td>5114</td>\n",
       "      <td>1193</td>\n",
       "      <td>2601</td>\n",
       "      <td>454</td>\n",
       "      <td>607</td>\n",
       "      <td>348</td>\n",
       "      <td>154</td>\n",
       "      <td>124</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>0.700598</td>\n",
       "      <td>0.663241</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.630477</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.631684</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.972661</td>\n",
       "      <td>5156</td>\n",
       "      <td>1281</td>\n",
       "      <td>274</td>\n",
       "      <td>55</td>\n",
       "      <td>2934</td>\n",
       "      <td>747</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>5268</td>\n",
       "      <td>1192</td>\n",
       "      <td>3208</td>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  train_auc  test_auc  train_accuracy  \\\n",
       "0                   AdaBoost   0.696973  0.655087        0.671543   \n",
       "1     Balanced_random_forest   1.000000  0.929390        0.998466   \n",
       "2                   CatBoost   0.984055  0.887409        0.930038   \n",
       "3          Gradient_boosting   0.955187  0.836820        0.865384   \n",
       "4        K_nearest_neighbors   0.907137  0.760620        0.839547   \n",
       "5        Logistic_regression   0.666568  0.645954        0.638390   \n",
       "6                Naive_bayes   0.619591  0.614055        0.583648   \n",
       "7              Random_forest   0.977650  0.876040        0.910217   \n",
       "8  Support_vector_classifier   0.700598  0.663241        0.640632   \n",
       "9                    XGBoost   1.000000  0.917080        1.000000   \n",
       "\n",
       "   test_accuracy  train_precision  test_precision  train_recall  test_recall  \\\n",
       "0       0.651727         0.677314        0.667145      0.900917     0.878491   \n",
       "1       0.860790         1.000000        0.890801      0.997532     0.884586   \n",
       "2       0.806981         0.914695        0.805601      0.978740     0.908866   \n",
       "3       0.751778         0.842758        0.754892      0.963176     0.889898   \n",
       "4       0.720149         0.844398        0.750560      0.909454     0.823839   \n",
       "5       0.635194         0.654976        0.652136      0.883827     0.886096   \n",
       "6       0.578089         0.710564        0.708645      0.556951     0.545950   \n",
       "7       0.777247         0.893953        0.774567      0.970768     0.905833   \n",
       "8       0.630477         0.637371        0.631684      0.978740     0.972661   \n",
       "9       0.856539         1.000000        0.870069      1.000000     0.905087   \n",
       "\n",
       "   train_true_positives  test_true_positives  train_true_negatives  \\\n",
       "0                  4746                 1157                   946   \n",
       "1                  5255                 1165                  3208   \n",
       "2                  5156                 1197                  2727   \n",
       "3                  5074                 1172                  2261   \n",
       "4                  4791                 1085                  2325   \n",
       "5                  4656                 1167                   755   \n",
       "6                  2934                  719                  2013   \n",
       "7                  5114                 1193                  2601   \n",
       "8                  5156                 1281                   274   \n",
       "9                  5268                 1192                  3208   \n",
       "\n",
       "   test_true_negatives  train_false_positives  test_false_positives  \\\n",
       "0                  224                   2262                   578   \n",
       "1                  659                      0                   143   \n",
       "2                  513                    481                   289   \n",
       "3                  421                    947                   381   \n",
       "4                  441                    883                   361   \n",
       "5                  179                   2453                   623   \n",
       "6                  506                   1195                   296   \n",
       "7                  454                    607                   348   \n",
       "8                   55                   2934                   747   \n",
       "9                  623                      0                   179   \n",
       "\n",
       "   train_false_negatives  test_false_negatives  train_positive_rate  \\\n",
       "0                    522                   160              0.62152   \n",
       "1                     13                   152              0.62152   \n",
       "2                    112                   120              0.62152   \n",
       "3                    194                   145              0.62152   \n",
       "4                    477                   232              0.62152   \n",
       "5                    612                   150              0.62152   \n",
       "6                   2334                   598              0.62152   \n",
       "7                    154                   124              0.62152   \n",
       "8                    112                    36              0.62152   \n",
       "9                      0                   125              0.62152   \n",
       "\n",
       "   test_positive_rate  \n",
       "0             0.62152  \n",
       "1             0.62152  \n",
       "2             0.62152  \n",
       "3             0.62152  \n",
       "4             0.62152  \n",
       "5             0.62152  \n",
       "6             0.62152  \n",
       "7             0.62152  \n",
       "8             0.62152  \n",
       "9             0.62152  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop the ROC/AUC function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def get_roc_auc_score(y_pred: np.array, y_true: np.array) -> np.array:\n",
    "\n",
    "    '''Function that returns the auc of the model being run with the functionality to \n",
    "    return the graph output which will be stored in Kedro outputs\n",
    "    \n",
    "        Args:\n",
    "    - y_pred: predictions from the machine learning model\n",
    "    - y_actual: actual values being predicted by the machine learning model\n",
    "    \n",
    "    Returns:\n",
    "    - auc: area under the curve; A higher AUC indicates better classification performance\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # calculate the tpr and fpr for the different threshold values of the classifier outputs: \n",
    "    fpr, tpr, thresholds = roc_curve(y_true = y_true, y_score = y_pred)\n",
    "\n",
    "    # AUC calculation:\n",
    "    auc_output = auc( x = fpr, y = tpr )\n",
    "\n",
    "    return auc_output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6682393503606587"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict class labels and probabilities\n",
    "y_pred = clf.predict(X_test)               # Predictions (0 or 1)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]         # Probabilities for class 1\n",
    "\n",
    "get_roc_auc_score(y_pred= y_proba, y_true = y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3695892 , 0.6304108 ],\n",
       "       [0.11141682, 0.88858318],\n",
       "       [0.57282355, 0.42717645],\n",
       "       ...,\n",
       "       [0.39719119, 0.60280881],\n",
       "       [0.40706066, 0.59293934],\n",
       "       [0.34883389, 0.65116611]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2119, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modifying the champion model selector to:\n",
    "# pick multiple potential champions for stacking\n",
    "# Pick a minimum threshold for the optimization target\n",
    "# Also solve for the optimization of the difference between train and test split with a min threshold applied\n",
    "\n",
    "\n",
    "def select_champion_model(models: pd.DataFrame, parameters: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the champion model based on the highest or lowest value of a given optimization target.\n",
    "    \n",
    "    Args:\n",
    "    - models: A DataFrame containing model performance metrics.\n",
    "    - optimization_target: The metric by which to select the champion model (e.g., 'test_precision', 'test_recall', 'test_false_positives').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_model: The row corresponding to the selected champion model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if parameters['optimization_target'] not in models.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target'] }' not found in model metrics. Choose from: {list(models.columns)}\")\n",
    "    \n",
    "    # Determine whether we want the highest or lowest value\n",
    "    # Assuming metrics  'precision', 'recall', 'accuracy' need the highest, and 'false positives', 'false negatives', etc. need the lowest\n",
    "    if 'precision' in parameters['optimization_target']  or 'recall' in parameters['optimization_target']  or 'accuracy' in parameters['optimization_target'] :\n",
    "        ascending = False  # We want to maximize these metrics\n",
    "   \n",
    "    else:\n",
    "        ascending = True  # For counts (e.g., false positives), we want the minimum value\n",
    "    \n",
    "\n",
    "    # Sort the models based on the optimization target\n",
    "    sorted_models = models.sort_values(by=parameters['optimization_target'] , ascending=ascending)\n",
    "\n",
    "    # Drop models that do no meet the specified performance threshold:\n",
    "    sorted_models = sorted_models[sorted_models[parameters['optimization_target']] > parameters['optimization_threshold']].reset_index(drop = True)\n",
    "    \n",
    "    # Return the top model (first row after sorting)\n",
    "    champion_model = pd.DataFrame(sorted_models.iloc[0]).reset_index()\n",
    "\n",
    "    champion_model.columns = ['element', 'value']\n",
    "    \n",
    "\n",
    "    return  champion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>element</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model</td>\n",
       "      <td>Support_vector_classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.640632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.630477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_precision</td>\n",
       "      <td>0.637371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_precision</td>\n",
       "      <td>0.631684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_recall</td>\n",
       "      <td>0.97874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_recall</td>\n",
       "      <td>0.972661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_true_positives</td>\n",
       "      <td>5156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_true_positives</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_true_negatives</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_true_negatives</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_false_positives</td>\n",
       "      <td>2934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_false_positives</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_false_negatives</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test_false_negatives</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_positive_rate</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_positive_rate</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  element                      value\n",
       "0                   model  Support_vector_classifier\n",
       "1          train_accuracy                   0.640632\n",
       "2           test_accuracy                   0.630477\n",
       "3         train_precision                   0.637371\n",
       "4          test_precision                   0.631684\n",
       "5            train_recall                    0.97874\n",
       "6             test_recall                   0.972661\n",
       "7    train_true_positives                       5156\n",
       "8     test_true_positives                       1281\n",
       "9    train_true_negatives                        274\n",
       "10    test_true_negatives                         55\n",
       "11  train_false_positives                       2934\n",
       "12   test_false_positives                        747\n",
       "13  train_false_negatives                        112\n",
       "14   test_false_negatives                         36\n",
       "15    train_positive_rate                    0.62152\n",
       "16     test_positive_rate                    0.62152"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_champion_model(models = summary_output, parameters = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_true_positives</th>\n",
       "      <th>test_true_positives</th>\n",
       "      <th>train_true_negatives</th>\n",
       "      <th>test_true_negatives</th>\n",
       "      <th>train_false_positives</th>\n",
       "      <th>test_false_positives</th>\n",
       "      <th>train_false_negatives</th>\n",
       "      <th>test_false_negatives</th>\n",
       "      <th>train_positive_rate</th>\n",
       "      <th>test_positive_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.671543</td>\n",
       "      <td>0.651727</td>\n",
       "      <td>0.677314</td>\n",
       "      <td>0.667145</td>\n",
       "      <td>0.900917</td>\n",
       "      <td>0.878491</td>\n",
       "      <td>4746</td>\n",
       "      <td>1157</td>\n",
       "      <td>946</td>\n",
       "      <td>224</td>\n",
       "      <td>2262</td>\n",
       "      <td>578</td>\n",
       "      <td>522</td>\n",
       "      <td>160</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.860319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892644</td>\n",
       "      <td>0.998291</td>\n",
       "      <td>0.881545</td>\n",
       "      <td>5259</td>\n",
       "      <td>1161</td>\n",
       "      <td>3208</td>\n",
       "      <td>662</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>9</td>\n",
       "      <td>156</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.806981</td>\n",
       "      <td>0.914695</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.908866</td>\n",
       "      <td>5156</td>\n",
       "      <td>1197</td>\n",
       "      <td>2727</td>\n",
       "      <td>513</td>\n",
       "      <td>481</td>\n",
       "      <td>289</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>0.865266</td>\n",
       "      <td>0.752721</td>\n",
       "      <td>0.842620</td>\n",
       "      <td>0.755570</td>\n",
       "      <td>0.963176</td>\n",
       "      <td>0.890656</td>\n",
       "      <td>5074</td>\n",
       "      <td>1173</td>\n",
       "      <td>2260</td>\n",
       "      <td>422</td>\n",
       "      <td>948</td>\n",
       "      <td>380</td>\n",
       "      <td>194</td>\n",
       "      <td>144</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>0.839547</td>\n",
       "      <td>0.720149</td>\n",
       "      <td>0.844398</td>\n",
       "      <td>0.750560</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.823839</td>\n",
       "      <td>4791</td>\n",
       "      <td>1085</td>\n",
       "      <td>2325</td>\n",
       "      <td>441</td>\n",
       "      <td>883</td>\n",
       "      <td>361</td>\n",
       "      <td>477</td>\n",
       "      <td>232</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>0.638390</td>\n",
       "      <td>0.635194</td>\n",
       "      <td>0.654976</td>\n",
       "      <td>0.652136</td>\n",
       "      <td>0.883827</td>\n",
       "      <td>0.886096</td>\n",
       "      <td>4656</td>\n",
       "      <td>1167</td>\n",
       "      <td>755</td>\n",
       "      <td>179</td>\n",
       "      <td>2453</td>\n",
       "      <td>623</td>\n",
       "      <td>612</td>\n",
       "      <td>150</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>0.583648</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.710564</td>\n",
       "      <td>0.708645</td>\n",
       "      <td>0.556951</td>\n",
       "      <td>0.545950</td>\n",
       "      <td>2934</td>\n",
       "      <td>719</td>\n",
       "      <td>2013</td>\n",
       "      <td>506</td>\n",
       "      <td>1195</td>\n",
       "      <td>296</td>\n",
       "      <td>2334</td>\n",
       "      <td>598</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>0.911161</td>\n",
       "      <td>0.773003</td>\n",
       "      <td>0.894287</td>\n",
       "      <td>0.771242</td>\n",
       "      <td>0.972097</td>\n",
       "      <td>0.902809</td>\n",
       "      <td>5121</td>\n",
       "      <td>1189</td>\n",
       "      <td>2602</td>\n",
       "      <td>449</td>\n",
       "      <td>606</td>\n",
       "      <td>353</td>\n",
       "      <td>147</td>\n",
       "      <td>128</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.630477</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.631684</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.972661</td>\n",
       "      <td>5156</td>\n",
       "      <td>1281</td>\n",
       "      <td>274</td>\n",
       "      <td>55</td>\n",
       "      <td>2934</td>\n",
       "      <td>747</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>5268</td>\n",
       "      <td>1192</td>\n",
       "      <td>3208</td>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  train_accuracy  test_accuracy  train_precision  \\\n",
       "0                   AdaBoost        0.671543       0.651727         0.677314   \n",
       "1     Balanced_random_forest        0.998938       0.860319         1.000000   \n",
       "2                   CatBoost        0.930038       0.806981         0.914695   \n",
       "3          Gradient_boosting        0.865266       0.752721         0.842620   \n",
       "4        K_nearest_neighbors        0.839547       0.720149         0.844398   \n",
       "5        Logistic_regression        0.638390       0.635194         0.654976   \n",
       "6                Naive_bayes        0.583648       0.578089         0.710564   \n",
       "7              Random_forest        0.911161       0.773003         0.894287   \n",
       "8  Support_vector_classifier        0.640632       0.630477         0.637371   \n",
       "9                    XGBoost        1.000000       0.856539         1.000000   \n",
       "\n",
       "   test_precision  train_recall  test_recall  train_true_positives  \\\n",
       "0        0.667145      0.900917     0.878491                  4746   \n",
       "1        0.892644      0.998291     0.881545                  5259   \n",
       "2        0.805601      0.978740     0.908866                  5156   \n",
       "3        0.755570      0.963176     0.890656                  5074   \n",
       "4        0.750560      0.909454     0.823839                  4791   \n",
       "5        0.652136      0.883827     0.886096                  4656   \n",
       "6        0.708645      0.556951     0.545950                  2934   \n",
       "7        0.771242      0.972097     0.902809                  5121   \n",
       "8        0.631684      0.978740     0.972661                  5156   \n",
       "9        0.870069      1.000000     0.905087                  5268   \n",
       "\n",
       "   test_true_positives  train_true_negatives  test_true_negatives  \\\n",
       "0                 1157                   946                  224   \n",
       "1                 1161                  3208                  662   \n",
       "2                 1197                  2727                  513   \n",
       "3                 1173                  2260                  422   \n",
       "4                 1085                  2325                  441   \n",
       "5                 1167                   755                  179   \n",
       "6                  719                  2013                  506   \n",
       "7                 1189                  2602                  449   \n",
       "8                 1281                   274                   55   \n",
       "9                 1192                  3208                  623   \n",
       "\n",
       "   train_false_positives  test_false_positives  train_false_negatives  \\\n",
       "0                   2262                   578                    522   \n",
       "1                      0                   140                      9   \n",
       "2                    481                   289                    112   \n",
       "3                    948                   380                    194   \n",
       "4                    883                   361                    477   \n",
       "5                   2453                   623                    612   \n",
       "6                   1195                   296                   2334   \n",
       "7                    606                   353                    147   \n",
       "8                   2934                   747                    112   \n",
       "9                      0                   179                      0   \n",
       "\n",
       "   test_false_negatives  train_positive_rate  test_positive_rate  \n",
       "0                   160              0.62152             0.62152  \n",
       "1                   156              0.62152             0.62152  \n",
       "2                   120              0.62152             0.62152  \n",
       "3                   144              0.62152             0.62152  \n",
       "4                   232              0.62152             0.62152  \n",
       "5                   150              0.62152             0.62152  \n",
       "6                   598              0.62152             0.62152  \n",
       "7                   128              0.62152             0.62152  \n",
       "8                    36              0.62152             0.62152  \n",
       "9                   125              0.62152             0.62152  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
