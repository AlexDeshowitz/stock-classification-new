{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# visualization tools:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# models:\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# evaluation functions:\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook goal: Setup a basic machine learning framework that cleans data, standardizes features,\n",
    "#  evaluates feature impt, shap values, and a myriad of ML algorithms\n",
    "# TODO: add the day-of-week as a feature\n",
    "# TODO: Add in target date versus historic reference dates\n",
    "# TODO: Add in volume-based feature functionality\n",
    "# TODO: Evaluate standardizing features per stock or one model per stock - may not be enough data realistically\n",
    "# TODO: Check bol-range-pct calculation - only giving zero value\n",
    "# TODO: Add profit point forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions:\n",
    "\n",
    "def clean_stock_data(dataframe: pd.DataFrame) -> pd.DataFrame :\n",
    "\n",
    "    '''removes nulls and in the future will be built out to do any additonal cleaning on the dataframe that is necessary\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing all of the potential features\n",
    "        parameters: \n",
    "            calculation_field: field on which all of the features are built\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataset that is ready to load into a machine learning framework\n",
    "    '''\n",
    "\n",
    "    #TODO: In pipeline write this output to the catalogue\n",
    "    # remove records the preceed the target period to have complete information:\n",
    "    dataframe = dataframe.dropna() \n",
    "    #dataframe = dataframe.reset_index(drop = True) # we won't reset the index for now for traceability back to the date, ticker combination later after training\n",
    "\n",
    "    # set the date as an index to us post-forecasting: This is a bad idea, come back to the concept\n",
    "    #dataframe.set_index(keys = 'date', verify_integrity = False, inplace = True) # verify integrity Fale to allow duplicates**\n",
    "    \n",
    "    # remove fields that will not be used as predictive features (can be hardcoded since dataframe structure will be the same):\n",
    "    dataframe = dataframe.drop(columns = [ 'date', 'high', 'low', 'open', 'volume', 'adj_close'])\n",
    "    \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def identify_fields_to_standardize(dataframe: pd.DataFrame, parameters: Dict) -> np.array :\n",
    "\n",
    "    '''creates a list of the continuous fields to standardize by dimension within the predictive model; NOTE: this is used within the standardizer\n",
    "    \n",
    "    Args:\n",
    "        dataframe: dataframe that contains all of the fields of interest to be used in the calculations\n",
    "        parameters:\n",
    "            continuous_feature_cutoff: ratio of unique values to record count to be used to codify continuous features -> removes records from the standardization process which don't have enough data to standardize (e.g., boolean)\n",
    "\n",
    "    Returns: list of continuous fields to use in the standardization process based on user's specifications of \"uniqueness\" threshold    \n",
    "\n",
    "    '''\n",
    "\n",
    "    numeric_fields = dataframe.select_dtypes(include = 'number').columns\n",
    "    records = len(dataframe)\n",
    "\n",
    "    record_summary = pd.DataFrame(dataframe[numeric_fields].nunique(), columns = ['unique_values'])\n",
    "    record_summary['rows_in_df'] = records\n",
    "    record_summary['value_to_record_ratio'] = record_summary['unique_values']/ record_summary['rows_in_df']\n",
    "\n",
    "    # filter for a threshold specified by the user:\n",
    "    record_summary = record_summary[record_summary['value_to_record_ratio'] > parameters['continuous_feature_cutoff']]\n",
    "\n",
    "    # remove percentage features # TODO: later add in functionality to remove percentage based features\n",
    "\n",
    "    return record_summary.index\n",
    "\n",
    "\n",
    "# Justification for approach on scaling - the argument can be made that since our approach will generalize movemements across multiple securities that we need to standardize each security to its own price range.  Therefore, any features with price-relative values will be scaled per the security's price values to avoid odd splits in tree-based algos\n",
    "# the concern with standardization is generally focused on not letting any one feature have considerably more weight in a model than another; however in this case, \n",
    "\n",
    "\n",
    "def standardize_continuous_features(dataframe: pd.DataFrame, parameters: Dict) -> pd.DataFrame:\n",
    "\n",
    "    '''function that identifies the continuious features in the dataframe and standardizes each feature by equity to enable scaling relative to each equity\n",
    "    \n",
    "    Args:\n",
    "        Dataframe: Pandas dataframe to be used in machine learning\n",
    "        Parameters:\n",
    "            stock_field: field indicating the stock for the window function to scan\n",
    "            calculation_field: field for which the target is being calculated (used for drop in main row merge)\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe: containing the standardized data fields\n",
    "    \n",
    "    '''\n",
    "\n",
    "    continuous_fields = list(identify_fields_to_standardize(dataframe = dataframe, parameters = parameters))\n",
    "\n",
    "    # add in the ticker for grouping next:\n",
    "    continuous_fields.append(parameters['stock_field'])\n",
    "\n",
    "    # downselect to the fields that will be used to standardize:\n",
    "    continuous_dataframe = dataframe[continuous_fields]\n",
    "\n",
    "    # calculate z-scores: --> Standardizes within each feature to scale accordingly\n",
    "    z_scores = (continuous_dataframe - continuous_dataframe.groupby(by = parameters['stock_field']).transform('mean')) / continuous_dataframe.groupby(by = parameters['stock_field']).transform('std')\n",
    "\n",
    "    # drop the null ticker (not needed post groupby): \n",
    "    z_scores.drop(columns = [ parameters['stock_field'], parameters['calculation_field'] ], inplace = True)\n",
    "\n",
    "    # rename the fields to indicate standardization:\n",
    "    z_scores.columns = z_scores.columns + '_standardized'\n",
    "\n",
    "    # drop original continuous fields # TODO: coming back after calculation checks:\n",
    "    if parameters['drop_original_fields'] == True:\n",
    "        continuous_fields.remove(parameters['stock_field'])\n",
    "        dataframe.drop(columns = continuous_fields, inplace = True)\n",
    "\n",
    "    # append the fields back into the core dataframe:\n",
    "    z_scores = pd.concat([dataframe, z_scores], axis = 1)\n",
    "\n",
    "    # remove the standardized target field:\n",
    "    z_scores.drop(columns = z_scores.columns[z_scores.columns.str.contains('target')][1], inplace = True)\n",
    "\n",
    "    # remove unnecessary items:\n",
    "    del continuous_fields, continuous_dataframe\n",
    "\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode_tickers(dataframe: pd.DataFrame, parameters: Dict) -> pd.DataFrame:\n",
    "\n",
    "    '''Returns one-hot encoded features to the predictive dataset NOTE: May not work, but this retains some of the information in the original dataframe while also potentially giving the global model a nudge\n",
    "       Note: we choose not to drop first for now, even though it's a trap; Can be used post processing or as model features\n",
    "    Args:\n",
    "        dataframe: core dataset that has been augmented with additional features\n",
    "        parameters:\n",
    "            stock_field: text field containing the 3 letter ticker of the dataset\n",
    "    Returns:   \n",
    "        dataframe with augmented columns\n",
    "    \n",
    "    '''\n",
    "\n",
    "    dataframe = pd.get_dummies(data = dataframe, prefix = \"ind\", columns = [parameters['stock_field']], drop_first = False)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def profile_target_variable(dataframe: pd.DataFrame, parameters: dict):\n",
    "\n",
    "\n",
    "    '''Function that looks at the target variable and creates an output for the user to review and decide whether rebalancing will help classification task\n",
    "    Args:\n",
    "        dataframe: Main resulting dataframe from all data conversion steps\n",
    "        parameters:\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    # isolate the target variable:\n",
    "    target_field = list(dataframe.columns[dataframe.columns.str.contains('target')])\n",
    "\n",
    "    # create simple value count outputs:\n",
    "    target_summary_table = pd.DataFrame(dataframe[target_field].value_counts()).reset_index()\n",
    "    target_summary_table.rename(columns = {0 : 'counts'}, inplace = True)\n",
    "    target_summary_table['proportion'] = target_summary_table['counts'] / target_summary_table['counts'].sum()\n",
    "\n",
    "    # create bargraph and save it:\n",
    "    ''' TODO : resolve ability to output a matplotlib plot in kedro catalog\n",
    "    sns.countplot(x=target_field, data=dataframe)\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.show() '''\n",
    "    target_field =', '.join(dataframe.columns[dataframe.columns.str.contains('target')].str.replace(r'\\[|\\]', ''))\n",
    "    positive_proportion = target_summary_table[target_summary_table[target_field].astype(int) == 1]['proportion'].to_list()\n",
    "   \n",
    "\n",
    "    print('Classification target: ' + str(target_field) + \" contains a class balance of: \" + str(positive_proportion) + \" in the positive case\")\n",
    "           \n",
    "\n",
    "    return target_summary_table # TODO: Write this to the catalogue as a reporting output for the users\n",
    "\n",
    "\n",
    "\n",
    "def create_training_test_splits(dataframe: pd.DataFrame, parameters: Dict) :\n",
    "\n",
    "    '''Function that splits out training and test sets for machine learning; for the purposes of this model the way we piose the problem allows for random train test split\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing only the target field and the features to be used by the classifier\n",
    "        parameters:\n",
    "            test_ratio: proportion of samples in the dataframe to be used as a test set once the models are tuned and evaluated\n",
    "    \n",
    "    Returns:\n",
    "        X_train: training set for use in ML process\n",
    "        X_test: test set to be held out until all cross-validation is completed\n",
    "        y_train: training set for target variables\n",
    "        y_test: test target to be held out until all cross-validation is completed\n",
    "\n",
    "    '''\n",
    "\n",
    "    # define Y and x:\n",
    "    target_feature = list(dataframe.columns[dataframe.columns.str.contains('target')])\n",
    "\n",
    "    y = dataframe[target_feature]\n",
    "    X = dataframe.drop(columns = target_feature)\n",
    "\n",
    "    # create the training and test splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=parameters['test_size'], random_state=parameters['seed'], stratify = y)\n",
    "\n",
    "    #y_train = y_train.values.ravel()\n",
    "    #y_test = y_test.values.ravel()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def custom_recall_score(confusion_matrix: np.array) -> np.int64 :\n",
    "\n",
    "    recall_value = confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[1,0])\n",
    "\n",
    "    return recall_value\n",
    "\n",
    "\n",
    "def custom_precision_score(confusion_matrix: np.array) -> np.int64 : \n",
    "\n",
    "    precision_value = confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1])\n",
    "\n",
    "    return precision_value\n",
    "\n",
    "\n",
    "def extract_feature_importances(X: np.array, model: BaseEstimator) -> pd.DataFrame:\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract feature importances: \n",
    "    feature_importances = model.feature_importances_\n",
    "    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "                                          'Feature': feature_names,\n",
    "                                          'Importance': feature_importances\n",
    "                                        }\n",
    "                                        ).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "\n",
    "def select_champion_model(models: pd.DataFrame, parameters: str):\n",
    "    \"\"\"\n",
    "    Selects the champion model based on the highest or lowest value of a given optimization target.\n",
    "    \n",
    "    Args:\n",
    "    - models: A DataFrame containing model performance metrics.\n",
    "    - optimization_target: The metric by which to select the champion model (e.g., 'test_precision', 'test_recall', 'test_false_positives').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_model: The row corresponding to the selected champion model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if parameters['optimization_target'] not in models.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target'] }' not found in model metrics. Choose from: {list(models.columns)}\")\n",
    "    \n",
    "    # Determine whether we want the highest or lowest value\n",
    "    # Assuming metrics like 'precision', 'recall', 'accuracy' need the highest, and 'false positives', 'false negatives', etc. need the lowest\n",
    "    if 'precision' in parameters['optimization_target']  or 'recall' in parameters['optimization_target']  or 'accuracy' in parameters['optimization_target'] :\n",
    "        ascending = False  # We want to maximize these metrics\n",
    "   \n",
    "    else:\n",
    "        ascending = True  # For counts (e.g., false positives), we want the minimum value\n",
    "    \n",
    "    # Sort the models based on the optimization target\n",
    "    sorted_models = models.sort_values(by=parameters['optimization_target'] , ascending=ascending)\n",
    "    \n",
    "    # Return the top model (first row after sorting)\n",
    "    champion_model = pd.DataFrame(sorted_models.iloc[0]).reset_index()\n",
    "\n",
    "    champion_model.columns = ['element', 'value']\n",
    "    \n",
    "\n",
    "    return champion_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = catalog.load('combined_modeling_input')\n",
    "df = pd.read_csv('../data/03_primary/combined_modeling_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the parameters for the model: \n",
    "\n",
    "parameters = {'continuous_feature_cutoff' : 0.6,\n",
    "              'stock_field' : 'ticker',\n",
    "              'calculation_field' : 'close',\n",
    "              'drop_original_fields' : True,\n",
    "              'drop_stock_field': True, # keep this fixed \n",
    "              'test_size' : 0.20, # proportion of the dataset held out as the test set\n",
    "              'seed' : 1187,\n",
    "              'cross_val_splits' : 5,\n",
    "              'c' : 1.0,\n",
    "              'kernel' : 'rbf',\n",
    "              'gamma' : 'scale',\n",
    "              'optimization_target' : 'test_false_positives', # other_useful_inputs: train_accuracy, test_accuracy, train_precision, train_recall, test_recall, true_positives, false_positives\n",
    "              # specify the algorithms to be used:\n",
    "              'classifiers': {\n",
    "                'Logistic_regression': {\n",
    "                    'class_path': 'sklearn.linear_model.LogisticRegression',\n",
    "                    'params': {'penalty': 'l2', 'C': 1.0, 'max_iter': 100000}\n",
    "                },\n",
    "                'Random_forest': {\n",
    "                    'class_path': 'sklearn.ensemble.RandomForestClassifier',\n",
    "                    'params': {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
    "                },\n",
    "                'Support_vector_classifier': {\n",
    "                    'class_path': 'sklearn.svm.SVC',\n",
    "                    'params': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'}\n",
    "                },\n",
    "                'XGBoost': {\n",
    "                    'class_path': 'xgboost.XGBClassifier',\n",
    "                    'params': {'use_label_encoder': False, 'eval_metric': 'logloss'}\n",
    "                },\n",
    "                'K_nearest_neighbors': {\n",
    "                    'class_path': 'sklearn.neighbors.KNeighborsClassifier',\n",
    "                    'params': {'n_neighbors': 5, 'weights': 'uniform'}\n",
    "                },\n",
    "                'Gradient_boosting': {\n",
    "                    'class_path': 'sklearn.ensemble.GradientBoostingClassifier',\n",
    "                    'params': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3}\n",
    "                },\n",
    "                'Naive_bayes': {\n",
    "                    'class_path': 'sklearn.naive_bayes.GaussianNB',\n",
    "                    'params': {}\n",
    "                },\n",
    "                'Balanced_random_forest': {\n",
    "                    'class_path': 'imblearn.ensemble.BalancedRandomForestClassifier',\n",
    "                    'params': {'n_estimators': 200, 'max_features': 'sqrt', 'sampling_strategy' : 'all', 'bootstrap' : False, 'replacement' : True}\n",
    "                },\n",
    "                'AdaBoost': {\n",
    "                    'class_path': 'sklearn.ensemble.AdaBoostClassifier',\n",
    "                    'params': {'n_estimators': 50, 'learning_rate': 1.0}\n",
    "                },\n",
    "                'CatBoost': {\n",
    "                    'class_path': 'catboost.CatBoostClassifier',\n",
    "                    'params': {'iterations': 100, 'learning_rate': 0.1, 'depth': 6, 'silent': True}\n",
    "                }\n",
    "            }\n",
    "\n",
    "}\n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the null values from the dataset and drop un-needed columns for the classifier:\n",
    "test = clean_stock_data(dataframe= df)\n",
    "# test: standardize features:\n",
    "test = standardize_continuous_features(dataframe = test, parameters = parameters)\n",
    "# one-hot encode: \n",
    "test = one_hot_encode_tickers(dataframe = test, parameters= parameters)\n",
    "# create training and test sets\n",
    "X_train, X_test, y_train, y_test = create_training_test_splits(dataframe=test, parameters= parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### - Function development HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### - Testing functions HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Logistic_regression classifier\n",
      "Training: Random_forest classifier\n",
      "Training: Support_vector_classifier classifier\n",
      "Training: XGBoost classifier\n",
      "Training: K_nearest_neighbors classifier\n",
      "Training: Gradient_boosting classifier\n",
      "Training: Naive_bayes classifier\n",
      "Training: Balanced_random_forest classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: AdaBoost classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: CatBoost classifier\n"
     ]
    }
   ],
   "source": [
    "detailed_output, summary_output, feat_impt, feat_impt_summary = train_models(X_train = X_train, X_test = X_test, y_train = y_train, y_test= y_test, parameters = parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detailed_output.iloc[15,1]\n",
    "\n",
    "#detailed_output.groupby(by = ['classifier_details']).train_accuracy.mean()\n",
    "detailed_output.head(20).to_clipboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_true_positives</th>\n",
       "      <th>test_true_positives</th>\n",
       "      <th>train_true_negatives</th>\n",
       "      <th>test_true_negatives</th>\n",
       "      <th>train_false_positives</th>\n",
       "      <th>test_false_positives</th>\n",
       "      <th>train_false_negatives</th>\n",
       "      <th>test_false_negatives</th>\n",
       "      <th>train_positive_rate</th>\n",
       "      <th>test_positive_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.754837</td>\n",
       "      <td>0.698443</td>\n",
       "      <td>0.759852</td>\n",
       "      <td>0.717144</td>\n",
       "      <td>0.885345</td>\n",
       "      <td>0.850409</td>\n",
       "      <td>4664</td>\n",
       "      <td>1120</td>\n",
       "      <td>1734</td>\n",
       "      <td>360</td>\n",
       "      <td>1474</td>\n",
       "      <td>442</td>\n",
       "      <td>604</td>\n",
       "      <td>197</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>0.993511</td>\n",
       "      <td>0.848989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905036</td>\n",
       "      <td>0.989560</td>\n",
       "      <td>0.845858</td>\n",
       "      <td>5213</td>\n",
       "      <td>1114</td>\n",
       "      <td>3208</td>\n",
       "      <td>685</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>55</td>\n",
       "      <td>203</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.806981</td>\n",
       "      <td>0.914695</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.908866</td>\n",
       "      <td>5156</td>\n",
       "      <td>1197</td>\n",
       "      <td>2727</td>\n",
       "      <td>513</td>\n",
       "      <td>481</td>\n",
       "      <td>289</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>0.865266</td>\n",
       "      <td>0.751305</td>\n",
       "      <td>0.842620</td>\n",
       "      <td>0.754410</td>\n",
       "      <td>0.963176</td>\n",
       "      <td>0.889898</td>\n",
       "      <td>5074</td>\n",
       "      <td>1172</td>\n",
       "      <td>2260</td>\n",
       "      <td>420</td>\n",
       "      <td>948</td>\n",
       "      <td>382</td>\n",
       "      <td>194</td>\n",
       "      <td>145</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>0.839547</td>\n",
       "      <td>0.720149</td>\n",
       "      <td>0.844398</td>\n",
       "      <td>0.750560</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.823839</td>\n",
       "      <td>4791</td>\n",
       "      <td>1085</td>\n",
       "      <td>2325</td>\n",
       "      <td>441</td>\n",
       "      <td>883</td>\n",
       "      <td>361</td>\n",
       "      <td>477</td>\n",
       "      <td>232</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>0.638390</td>\n",
       "      <td>0.635194</td>\n",
       "      <td>0.654976</td>\n",
       "      <td>0.652136</td>\n",
       "      <td>0.883827</td>\n",
       "      <td>0.886096</td>\n",
       "      <td>4656</td>\n",
       "      <td>1167</td>\n",
       "      <td>755</td>\n",
       "      <td>179</td>\n",
       "      <td>2453</td>\n",
       "      <td>623</td>\n",
       "      <td>612</td>\n",
       "      <td>150</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>0.583648</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.710564</td>\n",
       "      <td>0.708645</td>\n",
       "      <td>0.556951</td>\n",
       "      <td>0.545950</td>\n",
       "      <td>2934</td>\n",
       "      <td>719</td>\n",
       "      <td>2013</td>\n",
       "      <td>506</td>\n",
       "      <td>1195</td>\n",
       "      <td>296</td>\n",
       "      <td>2334</td>\n",
       "      <td>598</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>0.912223</td>\n",
       "      <td>0.779612</td>\n",
       "      <td>0.896040</td>\n",
       "      <td>0.778370</td>\n",
       "      <td>0.971527</td>\n",
       "      <td>0.902811</td>\n",
       "      <td>5118</td>\n",
       "      <td>1189</td>\n",
       "      <td>2614</td>\n",
       "      <td>463</td>\n",
       "      <td>594</td>\n",
       "      <td>339</td>\n",
       "      <td>150</td>\n",
       "      <td>128</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.630477</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.631684</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.972661</td>\n",
       "      <td>5156</td>\n",
       "      <td>1281</td>\n",
       "      <td>274</td>\n",
       "      <td>55</td>\n",
       "      <td>2934</td>\n",
       "      <td>747</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>5268</td>\n",
       "      <td>1192</td>\n",
       "      <td>3208</td>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  train_accuracy  test_accuracy  train_precision  \\\n",
       "0                   AdaBoost        0.754837       0.698443         0.759852   \n",
       "1     Balanced_random_forest        0.993511       0.848989         1.000000   \n",
       "2                   CatBoost        0.930038       0.806981         0.914695   \n",
       "3          Gradient_boosting        0.865266       0.751305         0.842620   \n",
       "4        K_nearest_neighbors        0.839547       0.720149         0.844398   \n",
       "5        Logistic_regression        0.638390       0.635194         0.654976   \n",
       "6                Naive_bayes        0.583648       0.578089         0.710564   \n",
       "7              Random_forest        0.912223       0.779612         0.896040   \n",
       "8  Support_vector_classifier        0.640632       0.630477         0.637371   \n",
       "9                    XGBoost        1.000000       0.856539         1.000000   \n",
       "\n",
       "   test_precision  train_recall  test_recall  train_true_positives  \\\n",
       "0        0.717144      0.885345     0.850409                  4664   \n",
       "1        0.905036      0.989560     0.845858                  5213   \n",
       "2        0.805601      0.978740     0.908866                  5156   \n",
       "3        0.754410      0.963176     0.889898                  5074   \n",
       "4        0.750560      0.909454     0.823839                  4791   \n",
       "5        0.652136      0.883827     0.886096                  4656   \n",
       "6        0.708645      0.556951     0.545950                  2934   \n",
       "7        0.778370      0.971527     0.902811                  5118   \n",
       "8        0.631684      0.978740     0.972661                  5156   \n",
       "9        0.870069      1.000000     0.905087                  5268   \n",
       "\n",
       "   test_true_positives  train_true_negatives  test_true_negatives  \\\n",
       "0                 1120                  1734                  360   \n",
       "1                 1114                  3208                  685   \n",
       "2                 1197                  2727                  513   \n",
       "3                 1172                  2260                  420   \n",
       "4                 1085                  2325                  441   \n",
       "5                 1167                   755                  179   \n",
       "6                  719                  2013                  506   \n",
       "7                 1189                  2614                  463   \n",
       "8                 1281                   274                   55   \n",
       "9                 1192                  3208                  623   \n",
       "\n",
       "   train_false_positives  test_false_positives  train_false_negatives  \\\n",
       "0                   1474                   442                    604   \n",
       "1                      0                   117                     55   \n",
       "2                    481                   289                    112   \n",
       "3                    948                   382                    194   \n",
       "4                    883                   361                    477   \n",
       "5                   2453                   623                    612   \n",
       "6                   1195                   296                   2334   \n",
       "7                    594                   339                    150   \n",
       "8                   2934                   747                    112   \n",
       "9                      0                   179                      0   \n",
       "\n",
       "   test_false_negatives  train_positive_rate  test_positive_rate  \n",
       "0                   197              0.62152             0.62152  \n",
       "1                   203              0.62152             0.62152  \n",
       "2                   120              0.62152             0.62152  \n",
       "3                   145              0.62152             0.62152  \n",
       "4                   232              0.62152             0.62152  \n",
       "5                   150              0.62152             0.62152  \n",
       "6                   598              0.62152             0.62152  \n",
       "7                   128              0.62152             0.62152  \n",
       "8                    36              0.62152             0.62152  \n",
       "9                   125              0.62152             0.62152  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a champion model: \n",
    "\n",
    "def select_champion_model(models: pd.DataFrame, parameters: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the champion model based on the highest or lowest value of a given optimization target.\n",
    "    \n",
    "    Args:\n",
    "    - models: A DataFrame containing model performance metrics.\n",
    "    - optimization_target: The metric by which to select the champion model (e.g., 'test_precision', 'test_recall', 'test_false_positives').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_model: The row corresponding to the selected champion model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if parameters['optimization_target'] not in models.columns:\n",
    "        raise ValueError(f\"'{parameters['optimization_target'] }' not found in model metrics. Choose from: {list(models.columns)}\")\n",
    "    \n",
    "    # Determine whether we want the highest or lowest value\n",
    "    # Assuming metrics like 'precision', 'recall', 'accuracy' need the highest, and 'false positives', 'false negatives', etc. need the lowest\n",
    "    if 'precision' in parameters['optimization_target']  or 'recall' in parameters['optimization_target']  or 'accuracy' in parameters['optimization_target'] :\n",
    "        ascending = False  # We want to maximize these metrics\n",
    "   \n",
    "    else:\n",
    "        ascending = True  # For counts (e.g., false positives), we want the minimum value\n",
    "    \n",
    "    # Sort the models based on the optimization target\n",
    "    sorted_models = models.sort_values(by=parameters['optimization_target'] , ascending=ascending)\n",
    "    \n",
    "    # Return the top model (first row after sorting)\n",
    "    champion_model = pd.DataFrame(sorted_models.iloc[0]).reset_index()\n",
    "\n",
    "    champion_model.columns = ['element', 'value']\n",
    "    \n",
    "\n",
    "    return champion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_model = select_champion_model(models = detailed_output, parameters= parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1    (DecisionTreeClassifier(max_features='sqrt', r...\\nName: value, dtype: object\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(champion_model[champion_model['element']=='classifier_details']['value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next to-dos:  \n",
    "1.) Add parameters for all Classifiers to the parameters model  \n",
    "TODO: Add in ROC/Precision-recall curve\n",
    "Add SMOTE  \n",
    "Add Train and test class balances\n",
    "2.) Add Select \"n\" best logic to the outputs\n",
    "3.) Add in feature importances and feature selection before modeling run\n",
    "3.) Add in Hypterparameter tuning\n",
    "4.) Run with more positions/equity holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# refactor ML function to dynamically import any SKlearn module:\n",
    "\n",
    "\n",
    "def dynamic_import(class_path: str):\n",
    "    \"\"\"Dynamically imports a class from its full path.\"\"\"\n",
    "    module_name, class_name = class_path.rsplit('.', 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)\n",
    "\n",
    "def train_models(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series, parameters: dict) -> pd.DataFrame:\n",
    "    \n",
    "    '''WRITE DOCUMENTATION'''\n",
    "    \n",
    "    # Convert y_train to a 1D array if it's a DataFrame\n",
    "    y_train = y_train.iloc[:, 0].values if isinstance(y_train, pd.DataFrame) else y_train.values\n",
    "    \n",
    "    # Store feature names from the DataFrame\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    model, classifier_details, fold = [], [], []\n",
    "    train_precisions, test_precisions = [], []\n",
    "    train_recalls, test_recalls = [], []\n",
    "    train_f_scores, test_f_scores = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    train_true_positives, test_true_positives = [], []\n",
    "    train_true_negatives, test_true_negatives = [], []\n",
    "    train_false_positives, test_false_positives = [], []\n",
    "    train_false_negatives, test_false_negatives = [], []\n",
    "\n",
    "    feature_importances_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through classifiers specified in the parameters\n",
    "    for clf_name, clf_info in parameters['classifiers'].items():\n",
    "        # Dynamically import and instantiate classifier\n",
    "        clf_class = dynamic_import(clf_info['class_path'])\n",
    "        clf_params = clf_info['params']\n",
    "        clf = clf_class(**clf_params)  # Initialize classifier with params\n",
    "\n",
    "        print(f'Training: {clf_name} classifier')\n",
    "\n",
    "        # Cross-validation loop\n",
    "        cv = StratifiedKFold(n_splits=parameters['cross_val_splits'], shuffle=True, random_state=parameters['seed']).split(X_train, y_train)\n",
    "\n",
    "        for k, (fold_train, fold_test) in enumerate(cv):\n",
    "            clf.fit(X_train.iloc[fold_train].values, y_train[fold_train])\n",
    "            \n",
    "            # Predictions\n",
    "            train_pred = clf.predict(X_train.iloc[fold_train].values)\n",
    "            test_pred = clf.predict(X_train.iloc[fold_test].values)\n",
    "\n",
    "            # Confusion matrices\n",
    "            train_confusion_matrix = confusion_matrix(y_train[fold_train], train_pred)\n",
    "            test_confusion_matrix = confusion_matrix(y_train[fold_test], test_pred)\n",
    "\n",
    "            # Accuracy\n",
    "            train_accuracy = clf.score(X_train.iloc[fold_train].values, y_train[fold_train])\n",
    "            test_accuracy = clf.score(X_train.iloc[fold_test].values, y_train[fold_test])\n",
    "\n",
    "            # Precision\n",
    "            train_precision = precision_score(y_train[fold_train], train_pred)\n",
    "            test_precision = precision_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # Recall\n",
    "            train_recall = recall_score(y_train[fold_train], train_pred)\n",
    "            test_recall = recall_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # F1 Score\n",
    "            train_f = f1_score(y_train[fold_train], train_pred)\n",
    "            test_f = f1_score(y_train[fold_test], test_pred)\n",
    "\n",
    "            # True/False Positives/Negatives\n",
    "            train_tp = train_confusion_matrix[1,1]\n",
    "            test_tp = test_confusion_matrix[1,1]\n",
    "            train_tn = train_confusion_matrix[0,0]\n",
    "            test_tn = test_confusion_matrix[0,0]\n",
    "            train_fp = train_confusion_matrix[0,1]\n",
    "            test_fp = test_confusion_matrix[0,1]\n",
    "            train_fn = train_confusion_matrix[1,0]\n",
    "            test_fn = test_confusion_matrix[1,0]\n",
    "\n",
    "            # Append metrics\n",
    "            model.append(clf_name)\n",
    "            classifier_details.append(clf)\n",
    "            fold.append(k)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            train_precisions.append(train_precision)\n",
    "            test_precisions.append(test_precision)\n",
    "            train_recalls.append(train_recall)\n",
    "            test_recalls.append(test_recall)\n",
    "            train_f_scores.append(train_f)\n",
    "            test_f_scores.append(test_f)\n",
    "            train_true_positives.append(train_tp)\n",
    "            test_true_positives.append(test_tp)\n",
    "            train_true_negatives.append(train_tn)\n",
    "            test_true_negatives.append(test_tn)\n",
    "            train_false_positives.append(train_fp)\n",
    "            test_false_positives.append(test_fp)\n",
    "            train_false_negatives.append(train_fn)\n",
    "            test_false_negatives.append(test_fn)\n",
    "\n",
    "            # Extract feature importances or coefficients\n",
    "            if hasattr(clf, \"coef_\"):  # For LogisticRegression\n",
    "                feature_importances = clf.coef_.flatten()\n",
    "                temp_df = pd.DataFrame({\n",
    "                    \"model\": [clf_name]*len(feature_importances),\n",
    "                    \"fold\": [k]*len(feature_importances),\n",
    "                    \"feature\": feature_names,\n",
    "                    \"importance\": feature_importances\n",
    "                })\n",
    "                feature_importances_df = pd.concat([feature_importances_df, temp_df], ignore_index=True)\n",
    "            \n",
    "            elif hasattr(clf, \"feature_importances_\"):  # For RandomForest, XGBoost\n",
    "                feature_importances = clf.feature_importances_\n",
    "                temp_df = pd.DataFrame({\n",
    "                    \"model\": [clf_name]*len(feature_importances),\n",
    "                    \"fold\": [k]*len(feature_importances),\n",
    "                    \"feature\": feature_names,\n",
    "                    \"importance\": feature_importances\n",
    "                })\n",
    "                feature_importances_df = pd.concat([feature_importances_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Summary of feature importances\n",
    "    feature_importances_summary_df = feature_importances_df.groupby(['model', 'feature']).agg(\n",
    "        importance_value=('importance', 'mean')).reset_index()\n",
    "\n",
    "    # Detailed results DataFrame\n",
    "    detailed_results_df = pd.DataFrame({\n",
    "        \"model\": model,\n",
    "        \"classifier_details\": classifier_details,\n",
    "        \"fold\": fold,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"test_accuracy\": test_accuracies,\n",
    "        \"train_precision\": train_precisions,\n",
    "        \"test_precision\": test_precisions,\n",
    "        \"train_recall\": train_recalls,\n",
    "        \"test_recall\": test_recalls,\n",
    "        \"train_true_positives\": train_true_positives,\n",
    "        \"test_true_positives\": test_true_positives,\n",
    "        \"train_true_negatives\": train_true_negatives,\n",
    "        \"test_true_negatives\": test_true_negatives,\n",
    "        \"train_false_positives\": train_false_positives,\n",
    "        \"test_false_positives\": test_false_positives,\n",
    "        \"train_false_negatives\": train_false_negatives,\n",
    "        \"test_false_negatives\": test_false_negatives\n",
    "    })\n",
    "\n",
    "    # Aggregated results\n",
    "    results_df = detailed_results_df.groupby('model').agg({\n",
    "        'train_accuracy': 'mean',\n",
    "        'test_accuracy': 'mean',\n",
    "        'train_precision': 'mean',\n",
    "        'test_precision': 'mean',\n",
    "        'train_recall': 'mean',\n",
    "        'test_recall': 'mean',\n",
    "        'train_true_positives': 'sum',\n",
    "        'test_true_positives': 'sum',\n",
    "        'train_true_negatives': 'sum',\n",
    "        'test_true_negatives': 'sum',\n",
    "        'train_false_positives': 'sum',\n",
    "        'test_false_positives': 'sum',\n",
    "        'train_false_negatives': 'sum',\n",
    "        'test_false_negatives': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Add in overall positve-class rates to the datasets:\n",
    "    results_df['train_positive_rate'] = (results_df['train_true_positives'] + results_df['train_false_negatives']) / (results_df['train_true_positives'] + results_df['train_false_negatives'] + results_df['train_false_positives'] + results_df['train_true_negatives'])\n",
    "    results_df['test_positive_rate'] = (results_df['test_true_positives'] + results_df['test_false_negatives']) / (results_df['test_true_positives'] + results_df['test_false_negatives'] + results_df['test_false_positives'] + results_df['test_true_negatives'])\n",
    "\n",
    "    # share message with complete:\n",
    "    print('training evaluation completed')\n",
    "    return detailed_results_df, results_df, feature_importances_df, feature_importances_summary_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Logistic_regression classifier\n",
      "Training: Random_forest classifier\n",
      "Training: Support_vector_classifier classifier\n",
      "Training: XGBoost classifier\n",
      "Training: K_nearest_neighbors classifier\n",
      "Training: Gradient_boosting classifier\n",
      "Training: Naive_bayes classifier\n",
      "Training: Balanced_random_forest classifier\n",
      "Training: AdaBoost classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/kedro-environment/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: CatBoost classifier\n",
      "training evaluation completed\n"
     ]
    }
   ],
   "source": [
    "# test out the new function outputs:\n",
    "\n",
    "detailed_output, summary_output, feat_impt, feat_impt_summary = train_models(X_train = X_train, X_test = X_test, y_train = y_train, y_test= y_test, parameters = parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_true_positives</th>\n",
       "      <th>test_true_positives</th>\n",
       "      <th>train_true_negatives</th>\n",
       "      <th>test_true_negatives</th>\n",
       "      <th>train_false_positives</th>\n",
       "      <th>test_false_positives</th>\n",
       "      <th>train_false_negatives</th>\n",
       "      <th>test_false_negatives</th>\n",
       "      <th>train_positive_rate</th>\n",
       "      <th>test_positive_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.754837</td>\n",
       "      <td>0.698443</td>\n",
       "      <td>0.759852</td>\n",
       "      <td>0.717144</td>\n",
       "      <td>0.885345</td>\n",
       "      <td>0.850409</td>\n",
       "      <td>4664</td>\n",
       "      <td>1120</td>\n",
       "      <td>1734</td>\n",
       "      <td>360</td>\n",
       "      <td>1474</td>\n",
       "      <td>442</td>\n",
       "      <td>604</td>\n",
       "      <td>197</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced_random_forest</td>\n",
       "      <td>0.992331</td>\n",
       "      <td>0.845686</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908564</td>\n",
       "      <td>0.987662</td>\n",
       "      <td>0.835983</td>\n",
       "      <td>5203</td>\n",
       "      <td>1101</td>\n",
       "      <td>3208</td>\n",
       "      <td>691</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>65</td>\n",
       "      <td>216</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.806981</td>\n",
       "      <td>0.914695</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.908866</td>\n",
       "      <td>5156</td>\n",
       "      <td>1197</td>\n",
       "      <td>2727</td>\n",
       "      <td>513</td>\n",
       "      <td>481</td>\n",
       "      <td>289</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>0.865384</td>\n",
       "      <td>0.752249</td>\n",
       "      <td>0.842758</td>\n",
       "      <td>0.755056</td>\n",
       "      <td>0.963176</td>\n",
       "      <td>0.890656</td>\n",
       "      <td>5074</td>\n",
       "      <td>1173</td>\n",
       "      <td>2261</td>\n",
       "      <td>421</td>\n",
       "      <td>947</td>\n",
       "      <td>381</td>\n",
       "      <td>194</td>\n",
       "      <td>144</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K_nearest_neighbors</td>\n",
       "      <td>0.839547</td>\n",
       "      <td>0.720149</td>\n",
       "      <td>0.844398</td>\n",
       "      <td>0.750560</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.823839</td>\n",
       "      <td>4791</td>\n",
       "      <td>1085</td>\n",
       "      <td>2325</td>\n",
       "      <td>441</td>\n",
       "      <td>883</td>\n",
       "      <td>361</td>\n",
       "      <td>477</td>\n",
       "      <td>232</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>0.638390</td>\n",
       "      <td>0.635194</td>\n",
       "      <td>0.654976</td>\n",
       "      <td>0.652136</td>\n",
       "      <td>0.883827</td>\n",
       "      <td>0.886096</td>\n",
       "      <td>4656</td>\n",
       "      <td>1167</td>\n",
       "      <td>755</td>\n",
       "      <td>179</td>\n",
       "      <td>2453</td>\n",
       "      <td>623</td>\n",
       "      <td>612</td>\n",
       "      <td>150</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive_bayes</td>\n",
       "      <td>0.583648</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.710564</td>\n",
       "      <td>0.708645</td>\n",
       "      <td>0.556951</td>\n",
       "      <td>0.545950</td>\n",
       "      <td>2934</td>\n",
       "      <td>719</td>\n",
       "      <td>2013</td>\n",
       "      <td>506</td>\n",
       "      <td>1195</td>\n",
       "      <td>296</td>\n",
       "      <td>2334</td>\n",
       "      <td>598</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>0.909981</td>\n",
       "      <td>0.774418</td>\n",
       "      <td>0.895187</td>\n",
       "      <td>0.774639</td>\n",
       "      <td>0.968681</td>\n",
       "      <td>0.899018</td>\n",
       "      <td>5103</td>\n",
       "      <td>1184</td>\n",
       "      <td>2610</td>\n",
       "      <td>457</td>\n",
       "      <td>598</td>\n",
       "      <td>345</td>\n",
       "      <td>165</td>\n",
       "      <td>133</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support_vector_classifier</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.630477</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.631684</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.972661</td>\n",
       "      <td>5156</td>\n",
       "      <td>1281</td>\n",
       "      <td>274</td>\n",
       "      <td>55</td>\n",
       "      <td>2934</td>\n",
       "      <td>747</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905087</td>\n",
       "      <td>5268</td>\n",
       "      <td>1192</td>\n",
       "      <td>3208</td>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0.62152</td>\n",
       "      <td>0.62152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  train_accuracy  test_accuracy  train_precision  \\\n",
       "0                   AdaBoost        0.754837       0.698443         0.759852   \n",
       "1     Balanced_random_forest        0.992331       0.845686         1.000000   \n",
       "2                   CatBoost        0.930038       0.806981         0.914695   \n",
       "3          Gradient_boosting        0.865384       0.752249         0.842758   \n",
       "4        K_nearest_neighbors        0.839547       0.720149         0.844398   \n",
       "5        Logistic_regression        0.638390       0.635194         0.654976   \n",
       "6                Naive_bayes        0.583648       0.578089         0.710564   \n",
       "7              Random_forest        0.909981       0.774418         0.895187   \n",
       "8  Support_vector_classifier        0.640632       0.630477         0.637371   \n",
       "9                    XGBoost        1.000000       0.856539         1.000000   \n",
       "\n",
       "   test_precision  train_recall  test_recall  train_true_positives  \\\n",
       "0        0.717144      0.885345     0.850409                  4664   \n",
       "1        0.908564      0.987662     0.835983                  5203   \n",
       "2        0.805601      0.978740     0.908866                  5156   \n",
       "3        0.755056      0.963176     0.890656                  5074   \n",
       "4        0.750560      0.909454     0.823839                  4791   \n",
       "5        0.652136      0.883827     0.886096                  4656   \n",
       "6        0.708645      0.556951     0.545950                  2934   \n",
       "7        0.774639      0.968681     0.899018                  5103   \n",
       "8        0.631684      0.978740     0.972661                  5156   \n",
       "9        0.870069      1.000000     0.905087                  5268   \n",
       "\n",
       "   test_true_positives  train_true_negatives  test_true_negatives  \\\n",
       "0                 1120                  1734                  360   \n",
       "1                 1101                  3208                  691   \n",
       "2                 1197                  2727                  513   \n",
       "3                 1173                  2261                  421   \n",
       "4                 1085                  2325                  441   \n",
       "5                 1167                   755                  179   \n",
       "6                  719                  2013                  506   \n",
       "7                 1184                  2610                  457   \n",
       "8                 1281                   274                   55   \n",
       "9                 1192                  3208                  623   \n",
       "\n",
       "   train_false_positives  test_false_positives  train_false_negatives  \\\n",
       "0                   1474                   442                    604   \n",
       "1                      0                   111                     65   \n",
       "2                    481                   289                    112   \n",
       "3                    947                   381                    194   \n",
       "4                    883                   361                    477   \n",
       "5                   2453                   623                    612   \n",
       "6                   1195                   296                   2334   \n",
       "7                    598                   345                    165   \n",
       "8                   2934                   747                    112   \n",
       "9                      0                   179                      0   \n",
       "\n",
       "   test_false_negatives  train_positive_rate  test_positive_rate  \n",
       "0                   197              0.62152             0.62152  \n",
       "1                   216              0.62152             0.62152  \n",
       "2                   120              0.62152             0.62152  \n",
       "3                   144              0.62152             0.62152  \n",
       "4                   232              0.62152             0.62152  \n",
       "5                   150              0.62152             0.62152  \n",
       "6                   598              0.62152             0.62152  \n",
       "7                   133              0.62152             0.62152  \n",
       "8                    36              0.62152             0.62152  \n",
       "9                   125              0.62152             0.62152  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
